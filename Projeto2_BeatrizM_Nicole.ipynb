{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beatriz Muniz de Castro e Silva\n",
    "\n",
    "Nicole Sarvasi Alves da Costa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o intuito de destacar mensagens que designem o nome do produto, primeiro baixamos os *tweets* com o nome do filme Bacurau. Ap√≥s, classificamos um por um entre \"relevante\" e \"irrelevantes\", tal classifica√ß√£o ser√° explicitada mais adiante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer isso, utilizaremos o algoritmo \"Naive Bayes\", criado no s√©culo XVIII. Atualmente, esse classificador probabil√≠stico tem sido utilizado na √°rea de *Machine Learning* principalmente para a categoriza√ß√£o de textos, determina√ß√£o de e-mails SPAM, e at√© para medicina diagn√≥stica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o treinamento do c√≥gigo, utilizamos uma base de 300 *tweets* para podermos implementar o classificador Naive-Bayes. Isso possibilitar√° verificarmos sua efici√™ncia, uma vez que poderemos comparar as classifica√ß√µes feitas, com as previamente estabelecidas por n√≥s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifica√ß√£o dos *tweets*:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-Relevante**: mensagens que passem uma imagem positiva sobre o filme                    \n",
    "    **ex.:** \"Bacurau √© um filme excelente!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-Irrelevante**: mensagens que passem uma imagem negativa sobre o filme ou que n√£o o caracterizem                   \n",
    "        **ex.:** \"O filme bacurau √© p√©ssimo\" ; \"Bacurau j√° est√° em cartaz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "#Instalando o pacote emoji para limpar mensagens\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#importando os pacotes necess√°rios\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import functools\n",
    "import operator\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: @nicknennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auth.pass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-3a68e2b1c78f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#leitura do arquivo no formato JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auth.pass'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auth.pass'"
     ]
    }
   ],
   "source": [
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#Identificador da conta no twitter: @nickcnennis\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'bacurau'\n",
    "\n",
    "#Quantidade m√≠nima de mensagens capturadas:\n",
    "n = 700\n",
    "#Quantidade m√≠nima de mensagens para a base de treinamento:\n",
    "t = 500\n",
    "\n",
    "#Filtro de l√≠ngua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#Inicia a captura\n",
    "i = 1\n",
    "msgs = []\n",
    "\n",
    "#retira os retweets\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode='extended').items():  \n",
    "    if (not msg.retweeted) and ('RT' not in msg.full_text): \n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um poss√≠vel vi√©s\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo n√£o existe para n√£o substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Classificamos os *tweets* de treinamento individualmente no Excel, atribuindo 0 para \"irrelevante\" e 1 para \"relevante\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leitura do arquivo excel\n",
    "treino = pd.read_excel(\"bacurau.xlsx\", \"Treinamento\") #tabela da parte de treinamento do excel\n",
    "teste = pd.read_excel(\"bacurau.xlsx\", \"Teste\") #tabela da parte de teste do excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a montagem do classificador Naive-Bayes, primeiramente limpamos os dados. Ou seja, corrigimos os espa√ßos entre as palavras e tiramos as pontua√ß√µes, tendo o cuidado de manter os emojis, os quais podem ser cruciais para determinar algum tipo de sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    import string\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if not word.startswith('https'))\n",
    "    \n",
    "    text_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "    text_split_whitespace = [substr.split() for substr in text_split_emoji]\n",
    "    text_split = functools.reduce(operator.concat, text_split_whitespace)\n",
    "    text = ' '.join(word for word in text_split)\n",
    "    \n",
    "    punctuation = '[!-/.:?;@]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    nova_linha = '[\\n]'\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed = re.sub(nova_linha, \" \", text_subbed)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, criamos uma lista com os *tweets* de teste limpos e aproveitamos para deixar todas as palavras em letra min√∫scula, o que nos ajudar√° no momento de identificar as frequ√™ncias das palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar uma lista com os tweets de teste limpos\n",
    "teste_t = pd.Series(teste[\"Teste\"])\n",
    "teste_limpo = []\n",
    "for i in teste_t:\n",
    "    teste_limpo.append(cleanup(i.lower()))\n",
    "# teste_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar uma lista com os tweets de treinamento limpos\n",
    "treino_s = pd.Series(treino[\"Treinamento\"])\n",
    "treino_limpo = []\n",
    "for i in treino_s:\n",
    "    treino_limpo.append(cleanup(str(i).lower()))\n",
    "# treino_limpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra limpeza que poderia ser feita para o melhor rendimento do nosso classificador seria a retirada de artigos, que n√£o se mostram cruciais para a classifica√ß√£o de sentimento de cada *tweet*. Para isso, seria eficaz uma fun√ß√£o a qual retirasse palavras de at√© duas letras, este processo serviria tamb√©m para a retirada de erros possivelmente feitos no momento de escrita nos textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequ√™ncia relativa total das palavras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau            0.059806\n",
       "de                 0.044272\n",
       "rt                 0.021100\n",
       "e                  0.021100\n",
       "a                  0.019288\n",
       "√©                  0.017864\n",
       "que                0.017735\n",
       "tirei              0.015534\n",
       "o                  0.015146\n",
       "eu                 0.012816\n",
       "voc√™               0.011133\n",
       "do                 0.009968\n",
       "ver                0.009191\n",
       "pra                0.008932\n",
       "no                 0.008026\n",
       "um                 0.007896\n",
       "assistir           0.007896\n",
       "filme              0.007249\n",
       "n√£o                0.007120\n",
       "uma                0.006861\n",
       "com                0.006731\n",
       "para               0.006731\n",
       "em                 0.006084\n",
       "da                 0.005955\n",
       "tem                0.005696\n",
       "gente              0.005437\n",
       "mas                0.005437\n",
       "qual               0.004919\n",
       "lunga              0.004531\n",
       "domingas           0.004531\n",
       "                     ...   \n",
       "lucasnunnes97      0.000129\n",
       "tem√°tica           0.000129\n",
       "cl√°ssico           0.000129\n",
       "üòü                  0.000129\n",
       "cair               0.000129\n",
       "finja              0.000129\n",
       "sagrado            0.000129\n",
       "Œ≤Œ±cœÖrŒ±œÖ            0.000129\n",
       "essas              0.000129\n",
       "melted             0.000129\n",
       "rua                0.000129\n",
       "conte              0.000129\n",
       "barata             0.000129\n",
       "amigas             0.000129\n",
       "r√°pido             0.000129\n",
       "dando              0.000129\n",
       "bacurit            0.000129\n",
       "conturbada         0.000129\n",
       "tanta              0.000129\n",
       "iskindolele        0.000129\n",
       "passado            0.000129\n",
       "chegaram           0.000129\n",
       "esp√©cie            0.000129\n",
       "promocional        0.000129\n",
       "felix              0.000129\n",
       "entendi            0.000129\n",
       "ansios√≠ssimaaaa    0.000129\n",
       "faca               0.000129\n",
       "companhia          0.000129\n",
       "adoro              0.000129\n",
       "Length: 1693, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino_t = (\" \").join(treino_limpo) #junta todos os tweets de treinamento em uma string s√≥\n",
    "lista_relativa = treino_t.split() #divide a string pra fazer uma lista com cada palavra separadamente\n",
    "frequencia_absoluta = pd.Series(lista_relativa).value_counts()\n",
    "frequencia_relativa = pd.Series(lista_relativa).value_counts(True)\n",
    "frequencia_relativa #frequencia relativa total das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequ√™ncia relativa dos *tweets* relevantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau           0.059778\n",
       "de                0.034291\n",
       "que               0.027340\n",
       "e                 0.024096\n",
       "o                 0.018999\n",
       "pra               0.018536\n",
       "ver               0.018072\n",
       "a                 0.016682\n",
       "assistir          0.014365\n",
       "filme             0.013438\n",
       "do                0.012975\n",
       "√©                 0.012975\n",
       "eu                0.012512\n",
       "no                0.010658\n",
       "n√£o               0.009268\n",
       "um                0.007414\n",
       "em                0.007414\n",
       "com               0.006951\n",
       "mas               0.006951\n",
       "cinema            0.006951\n",
       "me                0.006487\n",
       "mais              0.006487\n",
       "tem               0.005561\n",
       "bom               0.005097\n",
       "vou               0.005097\n",
       "sobre             0.005097\n",
       "uma               0.005097\n",
       "da                0.004634\n",
       "q                 0.004634\n",
       "vai               0.004634\n",
       "                    ...   \n",
       "pararem           0.000463\n",
       "sido              0.000463\n",
       "silveropereira    0.000463\n",
       "pre√ßo             0.000463\n",
       "gostou            0.000463\n",
       "exibidos          0.000463\n",
       "serem             0.000463\n",
       "indo              0.000463\n",
       "pelos             0.000463\n",
       "sombrios          0.000463\n",
       "festival          0.000463\n",
       "favorrt           0.000463\n",
       "top               0.000463\n",
       "lados             0.000463\n",
       "wotzik            0.000463\n",
       "trocado‚Äô          0.000463\n",
       "meter             0.000463\n",
       "ser√£o             0.000463\n",
       "leao              0.000463\n",
       "filha             0.000463\n",
       "vez               0.000463\n",
       "texto             0.000463\n",
       "estarei           0.000463\n",
       "mto               0.000463\n",
       "consigo           0.000463\n",
       "retrato           0.000463\n",
       "loucura           0.000463\n",
       "amanh√£            0.000463\n",
       "louca             0.000463\n",
       "adoro             0.000463\n",
       "Length: 878, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel = treino.index[treino.Relev√¢ncia == 1] #crit√©rio para selecionar s√≥ os tweets relevantes\n",
    "treino_r = treino.loc[rel, \"Treinamento\"]  #cria nova database com os tweets relevantes\n",
    "\n",
    "\n",
    "#frequ√™ncia das palavras nos tweets relevantes\n",
    "treino_rt = treino_r.str.cat()\n",
    "treino_rt = cleanup(treino_rt.lower())\n",
    "lista_relevante = treino_rt.split()\n",
    "frequencia_absoluta = pd.Series(lista_relevante).value_counts()\n",
    "frequencia_rel_relevantes = pd.Series(lista_relevante).value_counts(True)\n",
    "frequencia_rel_relevantes #frequ√™ncia relativa dos tweets relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequ√™ncia relativa dos *tweets* irrelevantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau           0.054954\n",
       "de                0.051858\n",
       "√©                 0.021091\n",
       "e                 0.020511\n",
       "a                 0.020511\n",
       "que               0.014899\n",
       "tirei             0.014319\n",
       "voc√™              0.014319\n",
       "o                 0.014125\n",
       "eu                0.011997\n",
       "do                0.009481\n",
       "para              0.009094\n",
       "um                0.008707\n",
       "uma               0.007933\n",
       "no                0.007546\n",
       "com               0.007159\n",
       "da                0.006966\n",
       "domingas          0.006772\n",
       "qual              0.006772\n",
       "lunga             0.006579\n",
       "n√£o               0.006385\n",
       "gente             0.006192\n",
       "em                0.005998\n",
       "tem               0.005998\n",
       "personagem        0.005998\n",
       "ver               0.005805\n",
       "assistir          0.005805\n",
       "pra               0.005611\n",
       "se                0.005418\n",
       "filme             0.005224\n",
       "                    ...   \n",
       "acionar           0.000193\n",
       "j√∫ri              0.000193\n",
       "f‚Ä¶rt              0.000193\n",
       "high              0.000193\n",
       "conta             0.000193\n",
       "putz              0.000193\n",
       "editor            0.000193\n",
       "silvero           0.000193\n",
       "descobrimos       0.000193\n",
       "al√©m              0.000193\n",
       "nadar             0.000193\n",
       "zadorarocha       0.000193\n",
       "camilamotad       0.000193\n",
       "oficialmente      0.000193\n",
       "grito             0.000193\n",
       "instagram         0.000193\n",
       "tony              0.000193\n",
       "davirochatirei    0.000193\n",
       "exposi√ß√£o         0.000193\n",
       "berrando          0.000193\n",
       "te‚Ä¶kkkkkk         0.000193\n",
       "aviso             0.000193\n",
       "colunista         0.000193\n",
       "querer            0.000193\n",
       "22h30             0.000193\n",
       "crise             0.000193\n",
       "chegamos          0.000193\n",
       "hojert            0.000193\n",
       "sanguevcs         0.000193\n",
       "üéº                 0.000193\n",
       "Length: 1244, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irel = treino.index[treino.Relev√¢ncia == 0]\n",
    "treino_ir = treino.loc[irel, \"Treinamento\"]  #tweets irrelevantes\n",
    "\n",
    "#frequ√™ncia das palavras nos tweets irrelevantes\n",
    "treino_irt = treino_ir.str.cat()\n",
    "treino_irt = cleanup(treino_irt.lower())\n",
    "lista_irrelevante = treino_irt.split()\n",
    "frequencia_absoluta = pd.Series(lista_irrelevante).value_counts()\n",
    "frequencia_rel_irrelevantes = pd.Series(lista_irrelevante).value_counts(True)\n",
    "frequencia_rel_irrelevantes #frequ√™ncia relativa dos tweets irrelevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinar as frequ√™ncias acima √© crucial para podermos calcular as probabilidades as quais ser√£o utilizadas no m√©todo de Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantidade de *tweets* relevantes e irrelevantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    354\n",
       "1    147\n",
       "Name: Relev√¢ncia, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantidade_relevancia = treino.Relev√¢ncia.value_counts()\n",
    "quantidade_relevancia #quantidade de tweets relevantes e irrelevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Porcentagem de *tweets* relevantes e irrelevantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    70.658683\n",
       "1    29.341317\n",
       "Name: Relev√¢ncia, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantidade_relevancia_pct = treino.Relev√¢ncia.value_counts(True)*100\n",
    "quantidade_relevancia_pct #porcentagem dos tweets que s√£o relevantes e irrelevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lista com todas as palavras que aparecem nos *tweets* da nossa tabela de treinamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nossa', 'eu', 'desviei', 'muito', 'r√°pido', 'de', 'um', 'spoiler', 'bacurau', 'me', 'senti', 'ninja', 'rt', 'avokdoido', 'quem', 'nao', 'gostou', '√©', 'no', 'm√≠nimo', 'do', 'pouco', 'nazista', 'sim', 'vi', 'ontem', 'com', 'a', 'viviane_cardoso', 'o', 'filme', 'fant√°stico', 'vale', 'pena', 'ver', 'se', 'voc√™', 'tiver', 'mente', 'aberta', 'e', 'qualquer', 'coisa', 'que', 'fa√ßo', 'vivi', 'sempre', 'bom', 'companheira', 'filmes', 'vida', 'lutas', 'lt', '3', 'hoje', 'finalmente', 'vejo', 'ser√°', 'consigo', 'chegar', 'at√©', 'sala', 'livre', 't√°', 'dif√≠cil', 'lzanin', 'texto', 'marcelo', 'coelho', 'chama', '‚Äòbolsonarisno', 'sinal', 'trocado‚Äô', 'lembra', 'ideia', 'alguns', 'jornalistas', 'na', '√©‚Ä¶', 'diretor', 'jo√£o', 'kleber', 'mendon√ßa', 'filho', 'n', 'vou', 'conseguir', 'qual', 'desse', 'q', 'galera', 'ta', 'doida', 'achei', 'era', 'peixe', 'isso', 'vai', 'assistir', 'sai', 'maluco', 'todo', 'dia', 'bora', 'galeres', 'aproveitando', 'pra', 'divulgar', 'melhor', '2019', 'personagem', 'tirei', 'lunga', 'a√≠', 'cara', 'j√°', 'assistiu', 'ùêâùêöÃÅ', 'ùêöùê¨ùê¨ùê¢ùê¨ùê≠ùê¢ùêÆ', 'ùêÅùêöùêúùêÆùê´ùêöùêÆ', 'jŒ¨', 'Œ±ssŒπsœÑŒπœÖ', 'Œ≤Œ±cœÖrŒ±œÖ', 'ùïµùñÜÃÅ', 'ùñÜùñòùñòùñéùñòùñôùñéùñö', 'ùï≠ùñÜùñàùñöùñóùñÜùñö', 'j·¥ÄÃÅ', '·¥Äss…™s·¥õ…™·¥ú', 'b·¥Ä·¥Ñ·¥ú Ä·¥Ä·¥úÔπñ', 'foda', 'omelete', 'ainda', 'foi', 'visto', 'por', 'mais', '130', 'mil', 'pessoas', 'diariosm', 'sucesso', 'p√∫blico', 'n√£o', 'deve', 'ser', 'exibido', 'nos', 'cinemas', 'santa', 'maria', 'aparentemente', 's√≥', 'ao', 'nesse', 'pa√≠s', 's√°bado', 'to', 'ansios√≠ssimaaaa', 'fds', 'passa', 'saindo', 'cedo', 'da', 'faculdade', 'correndo', 'kmendoncafilho', 'cine', 'arte', 'uff', 'uma', 'equipada', 'como', 'poucas', 'numa', 'universidade', 'p√∫blica', 'lotada', 'estudantes', 'para', 'brasi‚Ä¶', 'fazendo', 'propaganda', 'grandao', 'pro', 'uber', 'aqui', 'ele', 'disse', 'domingo', 'perfeito', 'poxa', 'algu√©m', 'leva', 'psousadealmeida', 'tenho', 'novo', 'tarantino', 'it', 'aaaaaa', 'favor', 'parem', 'falar', '√†s', '20', '40', 'chamem', 'sabe', 'bacurinha', 'thalitamari', '_meirabeatriz', 'menina', 'agora', 'tu', 'tava', 'falando', 'kkkkkk', 'apois', 'gente', 'faz', 'nem', 'treinamento', 'pq', 'esse', 'teria', 'oposi√ß√£o', 'queria', 'ter', 'dinheiro', 'wwwmlna', 'algum', 'homem', 'feminista', 'online', 'disposto', 'dar', 'professora', 'didatica', 'perguntando', 'turma', 'viu', 'mas', 'voc√™s', 'foram', 'gostei', 'tiduraes', 'em', '31¬∞', 'top', '250', 'letterboxd', 'lindo', 'comigo', 'anarcofino', 'minha', 'ex', 'cineasta', 'cheia', 'dos', 'contatos', 'manda', 'mensagem', 'digo', 'com‚Ä¶', 'hortawitch', 'assistindo', 'estou', 'vivendo', 'ou', 'apenas', 'curtindo', 'retwittando', 'repostando', 'coisas', 'sobre', 'baruchinha', 'tem', 'acho', 'amiga', 'pfvr', '2', 'pode', 'ganhar', 'pr√™mio', 'ano', 'beijos', 'sabia', 'karine', 'teles', 'dona', 'meu', 'c', 'susto', 'thread', 'deliciosa', 'tes√£o', 'estreia', '10de10', 'caazalberto', 'escreveu', 'momento', 'boca', 'garotada', 'incomodando', 'gado', 'üî•', 'üêÇ', 'review', 'completo', 'site', 'üëâ', 'affandre', 'tanto', 'invis√≠vel', 'ser√£o', 'exibidos', 'festival', 'toronto', 'grandes', 've√≠culos', 'temporada', 'premia√ß‚Ä¶', 'aconteceu', 'tudo', 'ruim', 'clap', 'existem', 'imperd√≠veis', 'pela', 'relev√¢ncia', 'tem√°tica', 'admir√°veis', 'pelo', 'significado', 'cultural', 'memor√°veis', 'serem', 'emblem√°ticos', 'tendo', 'monitoria', 'exposi√ß√£o', 'gringos', 'depois', 'matei', 'todos', 'sujei', 'os', 'quadros', 'sangue', 'tempos', 'sombrios', 'apreciar', 'boa', 'resist√™ncia', '07', 'setembro', 'po√ßos', 'caldas', 'estarei', 'l√°', 'ingressos', 'comprados', 'quero', 'mto', 'ir', 'caralhoo', 'preciso', 've', 'namoral', 'assistam', 'pqp', 'temia', 'veio', 'cin√©filo', 'perto', 'mim', 't√£o', 'more', 'bater', 'palma', 'tailand√™s', 'for', 'va', 'paz', 'casa', 'caralho', 'fica', 'come√ßa', 'pensar', 'cinema', 'cada', 'pensada', 'd√°', 'baixar', 'minhas', 'expectativas', 'porque', 'gosto', 'expectativa', 'alta', 'assim', 'ajuda', 'tempo', 'louvando', 'vcs', 'ficam', 'ai', 'oxewill94', 'vez', 'bicho', 'avengers', 'adoro', 'consumo', 'brasileiro', 'saco', 'cheio', 'pararem', 'fico', 'quando', 'falam', 'insistentemente', 'outra', 'diegoquaglia2', 'sei', 'odeio', 'black', 'mirror', 'compara√ß√£o', '‚Ä¶', 'constrangedor', 'existir', 'instagram', 'chamado', 'bacurau_memes', 'deus', 'fa√ßa', 'cair', 'tempi', 'ain', 'aguento', 'ouvir', 'esses', 'porra', 'entulhando', 'timeline', 'her√≥i', 'hollywood', 'ok', 'falo', 'nada', 'deixa', 'exaltar', 'daqui', 'pouquinho', 'tb', 'fazer', 'meus', 'amigos', 'assistirem', 'v√£o', 'vamos', 'la', 'mudar', 'pais', 'eh', 'possivel', 'eles', 'tenham', 'realmente', 'tirado', 'edir', 'macedo', 'passando', '50', 'semana', 'dias_ligia', 'ningu√©m', 'perguntou', 'ameei', 'amor', 'bolha', 'hype', 'est√°', 'maior', 'cmg', 'sido', 'lana', 'del', 'rey', 'harry', 'styles', 'errado', 'nenhum', 'deles', 'desisti', 'isto', 'capitulo', 'ova', 'apoiar', 'esganado', 'pelos', 'norte', 'americanos', 'kkkkkkkk', 'disseram', 'assisti', 'lo', 'vem', 'entrar', 'internet', 'alheia', 'outro', 'tbm', 'aquarius', 'renatoxavoso', 'kl√©ber', 'entre', 'coma', 'cu', 'toda', 'fam√≠lia', 'obrigado', 'demais', 'inuyalice', 'fui', '99', 'certeza', 'ia', 'sair', 'tiraram', 'yesterday', 'amigo', 'sess√£o', 'das', '9', 'teve', 'alguem', 'amanha', 'stargirllv', 'carai', 'b√™', 'indo', 'yuri_reinaldo', '16', 'horas', 'pensando', 'cinesiageek', 'deveria', 'elogio', 'tipo', 'm√≥', 'com√©dia', 'a√ß√£o', 'drama', 'it2', 'mt', 'ansiosa', 'principalmente', 'as', 'meninas', 'ü•≥', 'aula', 'vsffffffff', 'lugar', 'pros', 'lados', 'zona', 'madureira', 'shopping', 'rei', 'leao', 'programado', 'meier', 'sessao', '21h', 'noite', 'puta', 'excelente', 'saud√°vel', 'catarse', 'ali√°s', 'mesmo', 'oxigene', 'maoleskine', 'maratonar', 'breve', 'again', 'primeira', 'lan√ßamento', 'determina', 'quanto', 'cartaz', 'essa', 'lan√ßado', 'maravilhoso', 'mundo', 's√©rio', 'brasileiros', '‚ù§', 'Ô∏è', 'sopranine', 'ali', 'juntas', 'moskito', 'recebendo', 'mandar', 'grupo', 'dividido', 'amou', 'odiou', 'idosas', 'estavam', 'gostaram', 'admitiram', 'bem', 'feito', 'wotzik', 'brasil', 'urgente', 'vivam', 'necess√°rio', 'iskindolele', 'estava', 'obcecado', 'passava', '80', 'outros', 'torcia', 'falasse', 'sobre‚Ä¶', 'por√©m', 'afirmo', 'tranquilidade', 'milenio', 'veja', 'molena', 'incentive', 'nacional', 'meter', 'doido', 'logo', 'iguatemi', 'antes', 'fique', 'sem', 'minh', 'dando', 'palestra', 'pessoal', 'fez', 'trilha', 'sonora', 'mimos', 'pai', 'aquelacristiana', 'f√£s', '=', 'melhores', 'parece', 'infinitamente', 'interessante', 'rodrigoazo', 'olha', 'motivo', 'vermos', 'filma√ßo', 'ode', 'nosso', 'sobretudo', 'valente', 'povo', 'nordestino', 'sensa√ß√£o', 'reencontro', 'cultura', 'retrato', 'hist√≥rico', 'majestoso', 'viva', 'chegou', 'jundia√≠', 'sexo', 'sagrado', 'seu', 'corpo', 'templo', 'compartilh√°', 'louca', 'sozinha', 'msm', 'drag√£o', 'sigur_ross_', 'arrasta', 'maxxxramon', 'choque', 'tbt', 'assista', 'cr√≠tica', 'arrobanerd', 'longa', 'traz', 'reviravoltas', 'tirar', 'f√¥lego', 'paulo__junior__', 'ignorou', 'som', 'redor', 'principal', 'resistia', 'exatamente', 'rasteira', 'contradit√≥rio', 'nas', 'ideias', 'escreve', 'saber', 'met√°fora', 'p', 'gyn', 't√¥', 'lembrando', 'cena', 'fala', 'personagens', 'sob', 'efeito', 'forte', 'psicotr√≥pico', 'morrer', 'hauahuahuahuhauauuahuahuauuahuahhuahuauuahuahhuahuauuahuah', 'camisetinha', 'temos', 'so', '1', 'pessoa', 'reclamando', 'alguma', 'boyzinha', 'xuliaxx', 'zap', 'conversar', 'seguinte', 'eupalmeirensa', 'laianexx', 'laiane', 'quer', 'morenamoraes', 'abruxapreta', 'cinemarkoficial', 'querendo', 'salas', 'posso', 'abre', 'nenhuma', 'h4ckaq', 'dor', 'cabe√ßa', 'enjoo', 'vontade', 'ressaca', 'di√°logos', 'companhia', 'sincera', 'tomar', 'a√ßa√≠', 'verdade', 'beber', 'aff', 'sinopse', 'interessou', 'bastante', 'lucasnunnes97', '_msoliveira', 'processando', 'tanta', 'atualmente', 'gastei', 'chance', 'vendo', 'aquela', 'tranqueira', 'incr√≠vel', 'adjetivos', 'qualidade', 'deveriam', 'provavelmente', 'ja', 'cinemark', 'ent√£o', 'cuiabano', 'prestigiar', 'hein', 'faltava', 'unisse', 'todas', 'tribos', 'pablomoreno', 'amei', 'sert√£o', 'impacto', 'lucidez', 'realiza√ß√£o', 'precisa', 'digerir', 'ap√≥s', 'muitos', 'elogios', 'espero', 'encante', 'hj', 'assisto', 'refrescos', 'agenda', 'conturbada', 'par√ßa', '‚ô•', 'meio', 'loucura', 'assistido', 'mandei', 'tweet', 'mauromendoncaf', 'engravidar', 'louco', 'tesao', 'sendo', 'socorro', 'dizer', '‚Äúamiga', 'juntas‚Äù', 'supero', 'aniversario', 'paga', '√∫nica', 'entendi', 'ns', 'this', 'is', 'homofobia', 'filhudi', 'filhusi', 'brasilia', 'w', 'f√©', 'poder', 'fernanda', 'reservando', 's√£o', 'luiz', 'alunos', 'ü§ß', 'ü•∞', 'üò≠', '√≥bvio', 'enfiar', 'obra', 'inspira', 'maravilha', 'silveropereira', 'votei', 'assistiria', 'hora', 'penso', 'perplexo', 'recorte', 'irregular', 'obsess√µes', 'exporta√ß√£o', 'comentar', 'realidade', 'amigas', 'essas', 'hist√≥rias', 'itimalian', 'bacurit', 'kkkkk', 'aaa', 'filha', 'pois', 'trate', 'cinem', 'volta', 'agoraaaaaaa', 'viaverdeshop', 'amiguinho', 'queira', 'prefer√™ncia', 'segunda', 'quarta', 'pre√ßo', 'promocional', 'postando', 'plena', 'convic√ß√£o', 'flop', 'citar', 'correntes', 'al√©m', 'central', 'ela', 'amanh√£', 'mesa', 'buteco', 'centro', 'cmggg', 'algm', 'hd5trange', 'nice', 'comprando', 'ingresso', 'naquelas', 'm√°quinas', 'deu', 'gritar', 'film√£o', 'caray', 'onde', 'refletir', 'vir√°', 'escolas', 'museus', 'sentido', 'literal', 'figurado', 'tentar', 'recuperar', 'projeto', 'entregar', 'rever', 'dressasantiago_', 'diferenciado', 'chamaram', 'estamos', 'evoluindo', 'caionare4l', 'vim', 'humildemente', 'irem', 'assitir', 'perfeitooo', 'iamplanett', 'aten√ß√£o', '22', '09', 'passar', 'vit√≥ria', 'op√ß√£o', 'barata', 'local‚Ä¶', 'bacuraufilme', 'fa√ßam', 'teste', 'domingas', 'orgulho', 'objetivos', 'cumpridos', 'beareclama', 'kkkk', 'falta', 'pacote', 'teresa', 'pospunkcearense', 'tabacaria', 'imposs√≠vel', 'inagaki', 'virar', 'sou', 'via', 'davirocha', 'bluecoloredboy', 'conte', 'amo', 'socorr', 'kkkkkkk', 'tirou', 'pr√≥pria', 'cidade', 'amig√°vel', 'pac√≠fica', 'tentando', 'atrapalhar', 'sua', 'obrigada', 'pr√≥prias', 'r√©deas', 'destino', 'continuar', 'sobrevivendo', 'omegamark_xii', 'te', 'agostinho', 'carrara', 'matar', 'carara', 'matem√°tica', 'basica', 'marque', 'ami‚Ä¶', 'ifavmarkten', 'tive', 'pesquisar', 'google', 'vc', 'escrevendo', 'bacalhau', 'heinhein', 'sdds', 'umazinha', 'ficar', 'alterada', 'johnforfans', 'vamo', 'tarde', 'nadar', 'minas', 'dps', 'f1', 'comer', 'sorvete', 'conversando', 'yearofsilencce', 'mia', 'links', 'favoooorre', 'c0breira', 'easter', 'eggs', 'cobreiros', 'papai', 'respons√°vel', 'daquele', 'buraco', 'cia', 'cavaram', 'o‚Ä¶', 'ogrunhido', 'confundo', 'bator√©', 'personalidade', 'miss√£o', 'nesta', 'capaz', 'cima', 'cumprir', 'seus', 'seja', 'quais', 'forem', 'arrasa', 'visual', 'aplique', 'achando', '√≥timo', 'ouvi', 'uns', '‚Äúnossa', 'agora‚Äú', 'te‚Ä¶', 'contemplada', 'caravana', 'caiu', 'l√°grima', 'gulagcanavieiro', 'pontua', 'acuradamente', 'infelizmente', 'sudeste', 'merda', 'ps', 'yuribt', 'camisa', 'parmera', '2a', 'compartilhe', 'grande', 'abra√ßo', '‚ö°', 'ü§Ø', 'üõ∏', 'üç∑', 'capa', 'edi√ß√£o', 'cahiers', 'du', 'cin√©ma', 'revista', 'importante', 'traz‚Ä¶', 'refletindo', 'cakespacek', 'v√°', 'wiiz', 'pedrinhofonseca', 'primeiro', 'fim', 'r', '5mi', 'arrecadados', 'durante', 'grava√ß√µes', '800', 'empregos', 'gerados', 'lend√°rio', 'uuuuu', 'flu√≠', 'bacanal', 'perfeita', 'b_soviet', 'abrutaflor', 'piranha', 'galinha', 'tidal', '√©pico', 'greggui_', 'pen√∫ltimo', 'lista', 'produ√ß√£o', 'franco', 'venceu', 'j√∫ri', 'cannes', 'tornand‚Ä¶', 'ah', 'justo', 'claro', 'mtvbrasil', 'arrecada', '5', 'milh√£o', 'bilheteria', 'final', 'gt', 'voz', 'conhecimento', 'raz√£o', 'consulta', 'importa', 'verdadeira', 'inspira√ß√£o', 'banhadaaouro', 'bus√£o', 'madrugada', 'issoeomuso', 'resposta', 'moreirapaty', 'inveja', 'caso', 'inclui', 'tamb√©m', 'hahahahah', 'digno', 'choca', 'rondelicia', 'finja', 'aturar', 'viado', 'enaltecendo', 'riverdale', 'enaltecer', 'gepeto666', 'p_dromenezes', 'somos', 'regi√£o', 'tecnologicamente', 'avan√ßada', 'buscando', 'disputar', 'posi√ß√µes', 'regi√µes', 'cl√°ssico', 'xen√≥fobo', 'acham', 'diferenciada', 'paulista', 'carioca', 'testes_damassa', 'fiquei', 'vingadores', 'aviso', 'virmos', 'po‚Ä¶', 'babi', 'colen', 'üëèüèº', 'üíó', '‚Äúvoc√™', '‚Äù', 'ma', 'bosta', 'testinhos', 'buzzfeed', 'üíÅüèª', 'v√™', 'n4rja', 'palavra', 'entender', 'exemplo', 'b√™bada', 'ide‚Ä¶', 'm_i_n_u_s', 'acabei', 'tweets', 'aleatorios', 'sudestino', 'ate', 'bahia', 'nordeste', 'desmerecer', 'uahauahaha', 'axzgazzoni', 'num', 'aqueles', '√¥nibus', 'presentear', 'sweet', 'coffee', 'week', 'pr√≥xima', 'banco', 'camilamotad', 'sonia', 'braga', 'comprei', 'porta', 'crise', 'ansiedade', 'embora', 'chorando', 'caminho', 'td', 'partir', 'quinta', 'feira', 'sesc', 'rua', 'augusta', 'sp', 'projetado', '4k', 'sonz√£o', '30‚Ä¶', 'fiz', 'üòÇ', 'quest√£o', 'invas√µes', 'domina√ß√µes', 'viol√™ncia', 'parte', 'construiu', 'invadiu', 'popula√ß√£o', 'morava', 'dizimada', 'nome', 'civiliza√ß√£o', 'pesquisa', 'r√°pida', 'caralhoooo', 'cancelada', 'cad√™', 'chega', 'chernobabe', '‚Äúpo', 'assiste', 'bacurau‚Äù', 'olhe', 'peh', 'usando', 'sandalia', 'couro', 'midianinja', '‚Äúbacurau‚Äù', 'ilustra', '‚Äúcahiers', 'cin√©ma‚Äù', 'conceituada', 'francesa', 'conh‚Ä¶', 'vetromn', 'cidades', 'dnlnblgng', 'menos', 'respeito', 'g√≠ria', 'sexual', 'boto', 'pezinhos', 'fora', 'carlos', 'come√ßam', 'exibir', 'boazinha', 'esperar', 'ccamls', 'sobrevivend9', 'esperan√ßa', 'isura_eru', 'fanart', 'mikhaetc', 'conquistas', 'juri', 'jovem', 'transar', 'querer', 'assassinar', 'gringo', 'filadaputa', 'ldna', 'tinha', 'hahaha', 'impressionada', 'in‚Ä¶', 'mostra', 'derrotar', 'ataques', 'bolson√°rios', 'conversaafiada', 'inteira', 'carapu√ßa', 'serviu', 'dontcallmealf', 'acredito', 'piracicaba', 'enquanto', 'falacioso', 'segue', '2‚Ä¶', 'tjxciv', '24h', 'chamei', 'tia', 'combinamos', 'chegamos', 'descobrimos', 'virou', 'programa√ß√£o', 'resultado', 'passado', 'condena', 'netrlix', 'jdornelles', 'oficial', 'dispon√≠vel', 'spotify', 'deezer', 'apple', 'music', 'play', 'üéª', 'üé∫', 'ü•Å', 'üéº', '_', 'n√°usea', 'pulo_', 'beijar', 'dormir', 'acordar', 'feirinha', 'pq10', 'senhor', 'sondas', 'ü§©', 'üòç', 'putz', 'diria', 'candymel', 'üéµ', 'completa', 'escutar', 'fi‚Ä¶', 'izarcosta', 'fosse', 'acionar', 'meme', 'juliancampos', 'caf√©', 'recebi', 'sach√™', 'a√ß√∫car', 'entra', '200', 'üí•', 'deixe', 'f‚Ä¶', 'heisenboff', 'ultima', 'brasileira', 'este', 'nervosa', '~~bacurau', 'mirror~~', 'conseguindo', 'cineart', 'boulos', 'chegando', 'metr√≥polis', 'coincid√™ncia', 'segundo', 'votos', 'internacionais', 'cinco', 'd√©cada', 'parasita', 'aranha', 'aran‚Ä¶', 'tehpipimi', 'irmao', 'desde', '2014', 'reclama‚Ä¶', 'j0anacs', 'high', 'life', 'shecomesincolo1', '√≠cone', 'berrando', 'perguntas', 'bacurauverso', 'acabou', 'trailer', 'tv', 'lot√©rica', 'desses', 'reforma', 'previd√™ncia', 'algo', 'comassim', 'just√≠ssimo', 'mano', 'dessa', 'voltar', 'unha', 'homenagear', 'femesmo', 'bastou', 'for√ßar', 'associa√ß√£o', 'absolutamente', 'kkkkkkkkkkkkkkkkkkkkkkkkk', 'alveskath', 'certo', 'üòÅ', 'medo', 'levantar', '√°gua', 'cozinha', 'alicepa', 'imagina', 'branco', 'americano', 'diferente', 'torrents', '[eu', 'mesma', 'bibliotec√°ria', 'refer√™ncia]', 'bacurinho', 'dei', 'boas', 'risadas', 'parabens', 'üëç', 'kdstephanie', '√∫ltimos', 'sabendo', 'lidar', 'badgesucks', 'enxergarmos', 'prisma', 'sociedade', 'vivemos', 'iremos', 'compreender', 'proposta', '_o_gugga', 'igual', 'overdose', 'emocomrade', 'intenso', 'vitrine_filmes', 'mail', 'show', 'maca√©', 'pedi', 'passarem', 'üòü', 'jazzbfr', 'quase', 'nunca', 'custa', 'meia', 'obriga', 'presen√ßa', 'ela‚Ä¶', 'n√©', 'isent√£o', 'esp√©cie', 'tropa', 'elite', 'esquerda', 'uso', 'cat√°rtica‚Ä¶', 'playlist', 'virginiaamoon', '√©poca', 'araputanga', 'apelido', 'super', 'engra√ßado', 'co', 'descobrir', 'hahahahahahahahaha', 'taurina', 'pr√≥prio', '10', 'crime', 'camaradas', 'fan', 'merchand', 'ator', 'prefeito', 'tony', 'j√∫nior', 'curtiu', 'comentou', 'desenho', 'feliz', 'dms', 'rmovinup', 'amoooooo', 'euzinha', '‚ô°', 'diariope', 'mata', 'tudinho', 'faca', 'oficialmente', 'botei', 'foder', 'rainha', 'teco', 'beb√™s', 'zadorarocha', 'comprar', 'guarda', 'chuva', 'ielison_', 'al√¥', 'campina', 'hor√°rios', '15', '35', 'pamonha', 'curau', 'dou', 'passarinho', 'ddaenerys', '‚Äúassiste', 'pagar', 'limpar', 'disso', 'xenofobico', 'pe√ßo', 'ruimdrigo_', 'rima', 'cosmopolita', 'conquistando', 'jamais', 'esquece', 'suas', 'ra√≠zes', 'perde', 'liga√ß√£o', 'srra', 'esfor√ßo', 'dificuldade', 'cidad√£o', 'andreense', 'hor√°rio', '22h30', 'carro', 'qu√™', 'pobre', 'neg√≥cio', 'saiu', 'raivaaa', 'aquele', 'f', 'gregoriorrr', 'sulista', 'bola', 'tira', 'belissimo', 'sarro', 'sulist‚Ä¶', 'delicia', '√≥', 'capitalismo', 'matastes', 'grito', 'povoado', 'chegaram', 'masterclass', 'editor', 'facha', 'critiquei', 'esta', 'aaaaaaaaaaaa', 'parar', 'i', 'got', 'silvero', 'pereira', 'yanplm123', 'men', 'teu', 'insta', 'melted', 'v√≠deos', 'buzzfeedbrasil', 'quiz', 'honra', 'meupai', 'pega', 'come', 'paulomoreria', 'ciladapieper', 'anigos', 'paia√ßada', 'ahahahah', 'pior', 'concordo', 'resumo', 'vibes', 'vomite', 'saio', 'atirando', 'mesmooooo', 'vir', 'evildebora', 'evildani13', 'cansadae', 'juliexndrews', '√≥dio', 'pantanal', 'passada', 'tinha‚Ä¶', 'tatifers', 'papocultura', 'hist√≥ria', 'cobra', 'colunista', 'sihan', 'felix', 'blog', 'papo', '√†', 'torna', 'gosta', 'asas', 'longe', 'p√°ssaro', 'territ√≥rio', 'acaba', 'voltando', 'fiel', 'coragem', 'trouxa', 'inventa', 'encher', 'move', 'mundos', 'fundos', 'defender', 'entes', 'queridos', 'cert√≠ssimo', 'conta', 'detalhe', 'receber√°', 'dm', 'mandando', 'shipneide', 'mista', 'vanity', 'fair', 'excessivamente', 'estereotipados', 'pegou', 'shade', 'nan', '3hoje', '√©‚Ä¶bora', 'passasaindo', 'perfeitopoxa', 'bacuraurt', 'aaaaaame', 'bacuraumas', 'tarantinort', 'bacurauser√°', 'bacurauacho', 'novoalgu√©m', 'verthread', 'premia√ß‚Ä¶era', 'bacurauquero', 'bacuraucaralhoo', 'bacuraunamoral', 'pqpgente', 'caralhobacurau', 'd√°eu', 'bichobacurau', 'v√£omeu', 'semanart', 'deusna', 'verminha', 'delesit', 'alheiabacurau', '9alguem', 'caraitoda', 'bacurauvsffffffff', 'bacuraugente', 'feitort', 'sobre‚Ä¶quero', 'bacurauainda', 'nacionalacho', 'aindasexo', 'msmalgu√©m', 'favorrt', 'gynt√¥', 'hauahuahuahuhauauuahuahuauuahuahhuahuauuahuahhuahuauuahuahcamisetinha', 'nenhumart', 'umaqueria', 'affa', 'filma√ßoquero', 'bacurautanta', 'cinemahoje', 'heinat√©', 'conturbadabacurau', 'pqpe', 'deusningu√©m', 'superogalera', 'bacurauningu√©m', 'deusns', 'wtenho', 'bacuraueu', 'maisfernanda', 'istotoda', 'perplexocr√≠tica', 'amigasmt', 'cuminha', 'viaverdeshopalgum', 'bacuraus√≥', 'cmgggtudo', 'nicet√°', 'novovi', 'coisapreciso', 'simbacurau', 'filhon', 'bacurauqual', 'pa√≠srt', 'brasi‚Ä¶por', 'oposi√ß√£ort', 'bacurauprofessora', 'amigatendo', 'sanguevcs', '‚Ä¶o', 'istort', 'local‚Ä¶rt', 'paztirei', 'mimtodos', 'kkkks√≥', 'davirochatirei', 'ami‚Ä¶rt', 'conversandotirei', 'o‚Ä¶que', 'te‚Ä¶muito', 'merdakkkkkkk', 'traz‚Ä¶tirei', 'brasi‚Ä¶rt', 'ami‚Ä¶tirei', 'galinhatirei', 'tornand‚Ä¶voc√™', 'com‚Ä¶rt', 'bacuraujusto', 'hahahahahachei', 'sime', 'po‚Ä¶rt', 'traz‚Ä¶eu', 'te‚Ä¶kkkkkk', 'ide‚Ä¶rt', '√©‚Ä¶rt', 'te‚Ä¶rt', 'ontemrt', '30‚Ä¶2', 'galinhagente', 'courort', 'conh‚Ä¶tirei', 'bacurauvoc√™', 'sexualrt', 'te‚Ä¶meu', 'faculdadert', 'filadaputaonde', 'hojert', 'in‚Ä¶bacurau', 'conversaafiadatirei', '2‚Ä¶rt', 'bacurauchamei', 'netrlixtirei', 'ide‚Ä¶_', 'bacurautirei', 'sondas√©', 'bacuraua', 'brasi‚Ä¶quem', 'courotirei', 'fi‚Ä¶eu', 'f‚Ä¶rt', 'destinoeu', 'mirror~~menina', 'traz‚Ä¶algu√©m', 'aran‚Ä¶rt', 'reclama‚Ä¶rt', 'fi‚Ä¶rt', 'nacionalrt', 'falamtirei', 'bacurauversoporra', 'o‚Ä¶kkkkkkkkkkkkkkkkkkkkkkkkk', 'te‚Ä¶pensar', 'bacurinhacomo', 'destinov√°', 'kkkkkkkkrt', '‚Ä¶mas', 'intensofiz', 'in‚Ä¶rt', 'simrt', 'ela‚Ä¶nao', 'cat√°rtica‚Ä¶carai', 'perfeitao', 'nomemedo', 'davirochart', 'brasi‚Ä¶tirei', 'bacurauquem', 'carlostirei', 'affcomprar', 'comigort', '‚Ä¶pamonha', 'bacuraumais', 'passarinhort', 'ent√£ort', 'fi‚Ä¶ontem', 'rimaqual', 'ela‚Ä¶a', 'culturalrt', 'com‚Ä¶eu', 'raivaaabacurau', 'peixert', 'frt', 'sulist‚Ä¶tirei', 'courocaralho', 'faculdadeaaaaaaaaaaaa', 'kkkkkkkamei', 'fi‚Ä¶tirei', 'meupaibacurau', 'comert', 'ent√£otirei', 'anigosporra', 'doidort', 'te‚Ä¶tirei', 'cansadaert', 'tinha‚Ä¶qual', 'chuvart', 'curt', 'o‚Ä¶a']\n"
     ]
    }
   ],
   "source": [
    "palavras_totais = [] #lista com todas as palavras sem duplicatas\n",
    "\n",
    "for palavra in lista_relativa:\n",
    "    if palavra not in palavras_totais:\n",
    "        palavras_totais.append(palavra)\n",
    "for palavra in lista_relevante:\n",
    "    if palavra not in palavras_totais:\n",
    "        palavras_totais.append(palavra)\n",
    "for palavra in lista_irrelevante:\n",
    "    if palavra not in palavras_totais:\n",
    "        palavras_totais.append(palavra)    \n",
    "print(palavras_totais)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_irr={}\n",
    "freq_rel={}\n",
    "\n",
    "for palavra in palavras_totais:\n",
    "    freq_irr[palavra]=1\n",
    "    freq_rel[palavra]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nossa': 3,\n",
       " 'eu': 28,\n",
       " 'desviei': 1,\n",
       " 'muito': 6,\n",
       " 'r√°pido': 1,\n",
       " 'de': 75,\n",
       " 'um': 17,\n",
       " 'spoiler': 2,\n",
       " 'bacurau': 130,\n",
       " 'me': 15,\n",
       " 'senti': 1,\n",
       " 'ninja': 1,\n",
       " 'rt': 6,\n",
       " 'avokdoido': 2,\n",
       " 'quem': 8,\n",
       " 'nao': 5,\n",
       " 'gostou': 2,\n",
       " '√©': 29,\n",
       " 'no': 24,\n",
       " 'm√≠nimo': 3,\n",
       " 'do': 29,\n",
       " 'pouco': 2,\n",
       " 'nazista': 2,\n",
       " 'sim': 5,\n",
       " 'vi': 5,\n",
       " 'ontem': 6,\n",
       " 'com': 16,\n",
       " 'a': 37,\n",
       " 'viviane_cardoso': 2,\n",
       " 'o': 42,\n",
       " 'filme': 30,\n",
       " 'fant√°stico': 2,\n",
       " 'vale': 2,\n",
       " 'pena': 3,\n",
       " 'ver': 40,\n",
       " 'se': 6,\n",
       " 'voc√™': 7,\n",
       " 'tiver': 2,\n",
       " 'mente': 2,\n",
       " 'aberta': 2,\n",
       " 'e': 53,\n",
       " 'qualquer': 3,\n",
       " 'coisa': 8,\n",
       " 'que': 60,\n",
       " 'fa√ßo': 2,\n",
       " 'vivi': 2,\n",
       " 'sempre': 3,\n",
       " 'bom': 12,\n",
       " 'companheira': 2,\n",
       " 'filmes': 7,\n",
       " 'vida': 4,\n",
       " 'lutas': 2,\n",
       " 'lt': 2,\n",
       " '3': 1,\n",
       " 'hoje': 8,\n",
       " 'finalmente': 3,\n",
       " 'vejo': 2,\n",
       " 'ser√°': 2,\n",
       " 'consigo': 2,\n",
       " 'chegar': 2,\n",
       " 'at√©': 4,\n",
       " 'sala': 3,\n",
       " 'livre': 2,\n",
       " 't√°': 9,\n",
       " 'dif√≠cil': 2,\n",
       " 'lzanin': 2,\n",
       " 'texto': 2,\n",
       " 'marcelo': 2,\n",
       " 'coelho': 2,\n",
       " 'chama': 2,\n",
       " '‚Äòbolsonarisno': 2,\n",
       " 'sinal': 2,\n",
       " 'trocado‚Äô': 2,\n",
       " 'lembra': 2,\n",
       " 'ideia': 2,\n",
       " 'alguns': 2,\n",
       " 'jornalistas': 2,\n",
       " 'na': 8,\n",
       " '√©‚Ä¶': 1,\n",
       " 'diretor': 1,\n",
       " 'jo√£o': 1,\n",
       " 'kleber': 1,\n",
       " 'mendon√ßa': 2,\n",
       " 'filho': 2,\n",
       " 'n': 2,\n",
       " 'vou': 12,\n",
       " 'conseguir': 1,\n",
       " 'qual': 1,\n",
       " 'desse': 4,\n",
       " 'q': 11,\n",
       " 'galera': 2,\n",
       " 'ta': 5,\n",
       " 'doida': 2,\n",
       " 'achei': 2,\n",
       " 'era': 6,\n",
       " 'peixe': 1,\n",
       " 'isso': 3,\n",
       " 'vai': 11,\n",
       " 'assistir': 32,\n",
       " 'sai': 1,\n",
       " 'maluco': 1,\n",
       " 'todo': 4,\n",
       " 'dia': 4,\n",
       " 'bora': 2,\n",
       " 'galeres': 2,\n",
       " 'aproveitando': 1,\n",
       " 'pra': 41,\n",
       " 'divulgar': 2,\n",
       " 'melhor': 8,\n",
       " '2019': 2,\n",
       " 'personagem': 3,\n",
       " 'tirei': 2,\n",
       " 'lunga': 2,\n",
       " 'a√≠': 4,\n",
       " 'cara': 1,\n",
       " 'j√°': 9,\n",
       " 'assistiu': 2,\n",
       " 'ùêâùêöÃÅ': 1,\n",
       " 'ùêöùê¨ùê¨ùê¢ùê¨ùê≠ùê¢ùêÆ': 1,\n",
       " 'ùêÅùêöùêúùêÆùê´ùêöùêÆ': 1,\n",
       " 'jŒ¨': 1,\n",
       " 'Œ±ssŒπsœÑŒπœÖ': 1,\n",
       " 'Œ≤Œ±cœÖrŒ±œÖ': 1,\n",
       " 'ùïµùñÜÃÅ': 1,\n",
       " 'ùñÜùñòùñòùñéùñòùñôùñéùñö': 1,\n",
       " 'ùï≠ùñÜùñàùñöùñóùñÜùñö': 1,\n",
       " 'j·¥ÄÃÅ': 1,\n",
       " '·¥Äss…™s·¥õ…™·¥ú': 1,\n",
       " 'b·¥Ä·¥Ñ·¥ú Ä·¥Ä·¥úÔπñ': 1,\n",
       " 'foda': 4,\n",
       " 'omelete': 2,\n",
       " 'ainda': 9,\n",
       " 'foi': 6,\n",
       " 'visto': 2,\n",
       " 'por': 7,\n",
       " 'mais': 15,\n",
       " '130': 1,\n",
       " 'mil': 1,\n",
       " 'pessoas': 1,\n",
       " 'diariosm': 2,\n",
       " 'sucesso': 2,\n",
       " 'p√∫blico': 2,\n",
       " 'n√£o': 21,\n",
       " 'deve': 3,\n",
       " 'ser': 5,\n",
       " 'exibido': 2,\n",
       " 'nos': 5,\n",
       " 'cinemas': 3,\n",
       " 'santa': 2,\n",
       " 'maria': 2,\n",
       " 'aparentemente': 1,\n",
       " 's√≥': 9,\n",
       " 'ao': 5,\n",
       " 'nesse': 1,\n",
       " 'pa√≠s': 3,\n",
       " 's√°bado': 2,\n",
       " 'to': 8,\n",
       " 'ansios√≠ssimaaaa': 2,\n",
       " 'fds': 2,\n",
       " 'passa': 1,\n",
       " 'saindo': 1,\n",
       " 'cedo': 2,\n",
       " 'da': 11,\n",
       " 'faculdade': 4,\n",
       " 'correndo': 2,\n",
       " 'kmendoncafilho': 6,\n",
       " 'cine': 2,\n",
       " 'arte': 2,\n",
       " 'uff': 1,\n",
       " 'uma': 12,\n",
       " 'equipada': 1,\n",
       " 'como': 7,\n",
       " 'poucas': 1,\n",
       " 'numa': 2,\n",
       " 'universidade': 1,\n",
       " 'p√∫blica': 1,\n",
       " 'lotada': 1,\n",
       " 'estudantes': 1,\n",
       " 'para': 6,\n",
       " 'brasi‚Ä¶': 1,\n",
       " 'fazendo': 2,\n",
       " 'propaganda': 2,\n",
       " 'grandao': 2,\n",
       " 'pro': 3,\n",
       " 'uber': 2,\n",
       " 'aqui': 3,\n",
       " 'ele': 3,\n",
       " 'disse': 4,\n",
       " 'domingo': 2,\n",
       " 'perfeito': 1,\n",
       " 'poxa': 1,\n",
       " 'algu√©m': 10,\n",
       " 'leva': 5,\n",
       " 'psousadealmeida': 2,\n",
       " 'tenho': 2,\n",
       " 'novo': 8,\n",
       " 'tarantino': 4,\n",
       " 'it': 5,\n",
       " 'aaaaaa': 1,\n",
       " 'favor': 2,\n",
       " 'parem': 1,\n",
       " 'falar': 6,\n",
       " '√†s': 2,\n",
       " '20': 3,\n",
       " '40': 1,\n",
       " 'chamem': 2,\n",
       " 'sabe': 2,\n",
       " 'bacurinha': 2,\n",
       " 'thalitamari': 1,\n",
       " '_meirabeatriz': 1,\n",
       " 'menina': 1,\n",
       " 'agora': 5,\n",
       " 'tu': 2,\n",
       " 'tava': 4,\n",
       " 'falando': 4,\n",
       " 'kkkkkk': 2,\n",
       " 'apois': 1,\n",
       " 'gente': 8,\n",
       " 'faz': 3,\n",
       " 'nem': 4,\n",
       " 'treinamento': 1,\n",
       " 'pq': 6,\n",
       " 'esse': 9,\n",
       " 'teria': 1,\n",
       " 'oposi√ß√£o': 1,\n",
       " 'queria': 6,\n",
       " 'ter': 6,\n",
       " 'dinheiro': 3,\n",
       " 'wwwmlna': 5,\n",
       " 'algum': 1,\n",
       " 'homem': 1,\n",
       " 'feminista': 1,\n",
       " 'online': 1,\n",
       " 'disposto': 1,\n",
       " 'dar': 1,\n",
       " 'professora': 1,\n",
       " 'didatica': 1,\n",
       " 'perguntando': 1,\n",
       " 'turma': 1,\n",
       " 'viu': 3,\n",
       " 'mas': 16,\n",
       " 'voc√™s': 3,\n",
       " 'foram': 3,\n",
       " 'gostei': 2,\n",
       " 'tiduraes': 2,\n",
       " 'em': 17,\n",
       " '31¬∞': 2,\n",
       " 'top': 2,\n",
       " '250': 2,\n",
       " 'letterboxd': 2,\n",
       " 'lindo': 4,\n",
       " 'comigo': 8,\n",
       " 'anarcofino': 1,\n",
       " 'minha': 6,\n",
       " 'ex': 1,\n",
       " 'cineasta': 1,\n",
       " 'cheia': 2,\n",
       " 'dos': 3,\n",
       " 'contatos': 1,\n",
       " 'manda': 1,\n",
       " 'mensagem': 1,\n",
       " 'digo': 1,\n",
       " 'com‚Ä¶': 1,\n",
       " 'hortawitch': 2,\n",
       " 'assistindo': 2,\n",
       " 'estou': 4,\n",
       " 'vivendo': 2,\n",
       " 'ou': 2,\n",
       " 'apenas': 4,\n",
       " 'curtindo': 2,\n",
       " 'retwittando': 2,\n",
       " 'repostando': 2,\n",
       " 'coisas': 2,\n",
       " 'sobre': 12,\n",
       " 'baruchinha': 2,\n",
       " 'tem': 13,\n",
       " 'acho': 5,\n",
       " 'amiga': 3,\n",
       " 'pfvr': 2,\n",
       " '2': 5,\n",
       " 'pode': 2,\n",
       " 'ganhar': 2,\n",
       " 'pr√™mio': 2,\n",
       " 'ano': 3,\n",
       " 'beijos': 2,\n",
       " 'sabia': 3,\n",
       " 'karine': 2,\n",
       " 'teles': 2,\n",
       " 'dona': 2,\n",
       " 'meu': 8,\n",
       " 'c': 2,\n",
       " 'susto': 2,\n",
       " 'thread': 1,\n",
       " 'deliciosa': 2,\n",
       " 'tes√£o': 2,\n",
       " 'estreia': 2,\n",
       " '10de10': 2,\n",
       " 'caazalberto': 2,\n",
       " 'escreveu': 2,\n",
       " 'momento': 2,\n",
       " 'boca': 2,\n",
       " 'garotada': 2,\n",
       " 'incomodando': 2,\n",
       " 'gado': 2,\n",
       " 'üî•': 2,\n",
       " 'üêÇ': 2,\n",
       " 'review': 2,\n",
       " 'completo': 2,\n",
       " 'site': 2,\n",
       " 'üëâ': 2,\n",
       " 'affandre': 2,\n",
       " 'tanto': 5,\n",
       " 'invis√≠vel': 2,\n",
       " 'ser√£o': 2,\n",
       " 'exibidos': 2,\n",
       " 'festival': 2,\n",
       " 'toronto': 2,\n",
       " 'grandes': 2,\n",
       " 've√≠culos': 2,\n",
       " 'temporada': 2,\n",
       " 'premia√ß‚Ä¶': 1,\n",
       " 'aconteceu': 3,\n",
       " 'tudo': 6,\n",
       " 'ruim': 3,\n",
       " 'clap': 4,\n",
       " 'existem': 2,\n",
       " 'imperd√≠veis': 2,\n",
       " 'pela': 2,\n",
       " 'relev√¢ncia': 2,\n",
       " 'tem√°tica': 2,\n",
       " 'admir√°veis': 2,\n",
       " 'pelo': 6,\n",
       " 'significado': 2,\n",
       " 'cultural': 2,\n",
       " 'memor√°veis': 2,\n",
       " 'serem': 2,\n",
       " 'emblem√°ticos': 2,\n",
       " 'tendo': 1,\n",
       " 'monitoria': 1,\n",
       " 'exposi√ß√£o': 1,\n",
       " 'gringos': 2,\n",
       " 'depois': 3,\n",
       " 'matei': 1,\n",
       " 'todos': 2,\n",
       " 'sujei': 1,\n",
       " 'os': 8,\n",
       " 'quadros': 1,\n",
       " 'sangue': 1,\n",
       " 'tempos': 2,\n",
       " 'sombrios': 2,\n",
       " 'apreciar': 2,\n",
       " 'boa': 3,\n",
       " 'resist√™ncia': 3,\n",
       " '07': 2,\n",
       " 'setembro': 2,\n",
       " 'po√ßos': 2,\n",
       " 'caldas': 2,\n",
       " 'estarei': 2,\n",
       " 'l√°': 3,\n",
       " 'ingressos': 2,\n",
       " 'comprados': 2,\n",
       " 'quero': 8,\n",
       " 'mto': 2,\n",
       " 'ir': 5,\n",
       " 'caralhoo': 1,\n",
       " 'preciso': 6,\n",
       " 've': 2,\n",
       " 'namoral': 1,\n",
       " 'assistam': 3,\n",
       " 'pqp': 1,\n",
       " 'temia': 2,\n",
       " 'veio': 3,\n",
       " 'cin√©filo': 2,\n",
       " 'perto': 2,\n",
       " 'mim': 3,\n",
       " 't√£o': 6,\n",
       " 'more': 2,\n",
       " 'bater': 2,\n",
       " 'palma': 2,\n",
       " 'tailand√™s': 2,\n",
       " 'for': 2,\n",
       " 'va': 2,\n",
       " 'paz': 2,\n",
       " 'casa': 3,\n",
       " 'caralho': 1,\n",
       " 'fica': 4,\n",
       " 'come√ßa': 2,\n",
       " 'pensar': 2,\n",
       " 'cinema': 16,\n",
       " 'cada': 2,\n",
       " 'pensada': 2,\n",
       " 'd√°': 1,\n",
       " 'baixar': 2,\n",
       " 'minhas': 3,\n",
       " 'expectativas': 2,\n",
       " 'porque': 2,\n",
       " 'gosto': 2,\n",
       " 'expectativa': 2,\n",
       " 'alta': 2,\n",
       " 'assim': 4,\n",
       " 'ajuda': 2,\n",
       " 'tempo': 6,\n",
       " 'louvando': 2,\n",
       " 'vcs': 2,\n",
       " 'ficam': 2,\n",
       " 'ai': 3,\n",
       " 'oxewill94': 1,\n",
       " 'vez': 2,\n",
       " 'bicho': 1,\n",
       " 'avengers': 2,\n",
       " 'adoro': 2,\n",
       " 'consumo': 2,\n",
       " 'brasileiro': 6,\n",
       " 'saco': 3,\n",
       " 'cheio': 3,\n",
       " 'pararem': 2,\n",
       " 'fico': 3,\n",
       " 'quando': 4,\n",
       " 'falam': 2,\n",
       " 'insistentemente': 2,\n",
       " 'outra': 2,\n",
       " 'diegoquaglia2': 1,\n",
       " 'sei': 3,\n",
       " 'odeio': 1,\n",
       " 'black': 2,\n",
       " 'mirror': 2,\n",
       " 'compara√ß√£o': 1,\n",
       " '‚Ä¶': 1,\n",
       " 'constrangedor': 1,\n",
       " 'existir': 1,\n",
       " 'instagram': 2,\n",
       " 'chamado': 1,\n",
       " 'bacurau_memes': 1,\n",
       " 'deus': 4,\n",
       " 'fa√ßa': 2,\n",
       " 'cair': 2,\n",
       " 'tempi': 2,\n",
       " 'ain': 2,\n",
       " 'aguento': 4,\n",
       " 'ouvir': 3,\n",
       " 'esses': 2,\n",
       " 'porra': 2,\n",
       " 'entulhando': 2,\n",
       " 'timeline': 3,\n",
       " 'her√≥i': 2,\n",
       " 'hollywood': 2,\n",
       " 'ok': 2,\n",
       " 'falo': 3,\n",
       " 'nada': 3,\n",
       " 'deixa': 3,\n",
       " 'exaltar': 2,\n",
       " 'daqui': 2,\n",
       " 'pouquinho': 2,\n",
       " 'tb': 3,\n",
       " 'fazer': 3,\n",
       " 'meus': 2,\n",
       " 'amigos': 2,\n",
       " 'assistirem': 3,\n",
       " 'v√£o': 2,\n",
       " 'vamos': 5,\n",
       " 'la': 3,\n",
       " 'mudar': 3,\n",
       " 'pais': 2,\n",
       " 'eh': 4,\n",
       " 'possivel': 2,\n",
       " 'eles': 2,\n",
       " 'tenham': 2,\n",
       " 'realmente': 2,\n",
       " 'tirado': 2,\n",
       " 'edir': 2,\n",
       " 'macedo': 2,\n",
       " 'passando': 3,\n",
       " '50': 2,\n",
       " 'semana': 8,\n",
       " 'dias_ligia': 2,\n",
       " 'ningu√©m': 4,\n",
       " 'perguntou': 4,\n",
       " 'ameei': 2,\n",
       " 'amor': 3,\n",
       " 'bolha': 2,\n",
       " 'hype': 2,\n",
       " 'est√°': 3,\n",
       " 'maior': 2,\n",
       " 'cmg': 2,\n",
       " 'sido': 2,\n",
       " 'lana': 2,\n",
       " 'del': 2,\n",
       " 'rey': 2,\n",
       " 'harry': 2,\n",
       " 'styles': 2,\n",
       " 'errado': 2,\n",
       " 'nenhum': 5,\n",
       " 'deles': 1,\n",
       " 'desisti': 1,\n",
       " 'isto': 1,\n",
       " 'capitulo': 2,\n",
       " 'ova': 2,\n",
       " 'apoiar': 3,\n",
       " 'esganado': 2,\n",
       " 'pelos': 2,\n",
       " 'norte': 3,\n",
       " 'americanos': 2,\n",
       " 'kkkkkkkk': 2,\n",
       " 'disseram': 2,\n",
       " 'assisti': 4,\n",
       " 'lo': 3,\n",
       " 'vem': 3,\n",
       " 'entrar': 2,\n",
       " 'internet': 3,\n",
       " 'alheia': 1,\n",
       " 'outro': 2,\n",
       " 'tbm': 4,\n",
       " 'aquarius': 3,\n",
       " 'renatoxavoso': 2,\n",
       " 'kl√©ber': 2,\n",
       " 'entre': 4,\n",
       " 'coma': 2,\n",
       " 'cu': 2,\n",
       " 'toda': 2,\n",
       " 'fam√≠lia': 2,\n",
       " 'obrigado': 2,\n",
       " 'demais': 2,\n",
       " 'inuyalice': 2,\n",
       " 'fui': 3,\n",
       " '99': 3,\n",
       " 'certeza': 2,\n",
       " 'ia': 2,\n",
       " 'sair': 2,\n",
       " 'tiraram': 3,\n",
       " 'yesterday': 3,\n",
       " 'amigo': 3,\n",
       " 'sess√£o': 6,\n",
       " 'das': 3,\n",
       " '9': 3,\n",
       " 'teve': 2,\n",
       " 'alguem': 4,\n",
       " 'amanha': 2,\n",
       " 'stargirllv': 2,\n",
       " 'carai': 1,\n",
       " 'b√™': 2,\n",
       " 'indo': 2,\n",
       " 'yuri_reinaldo': 2,\n",
       " '16': 2,\n",
       " 'horas': 3,\n",
       " 'pensando': 2,\n",
       " 'cinesiageek': 2,\n",
       " 'deveria': 3,\n",
       " 'elogio': 2,\n",
       " 'tipo': 3,\n",
       " 'm√≥': 2,\n",
       " 'com√©dia': 2,\n",
       " 'a√ß√£o': 2,\n",
       " 'drama': 2,\n",
       " 'it2': 4,\n",
       " 'mt': 2,\n",
       " 'ansiosa': 2,\n",
       " 'principalmente': 2,\n",
       " 'as': 5,\n",
       " 'meninas': 2,\n",
       " 'ü•≥': 2,\n",
       " 'aula': 2,\n",
       " 'vsffffffff': 1,\n",
       " 'lugar': 2,\n",
       " 'pros': 2,\n",
       " 'lados': 2,\n",
       " 'zona': 2,\n",
       " 'madureira': 2,\n",
       " 'shopping': 2,\n",
       " 'rei': 2,\n",
       " 'leao': 2,\n",
       " 'programado': 2,\n",
       " 'meier': 2,\n",
       " 'sessao': 2,\n",
       " '21h': 2,\n",
       " 'noite': 4,\n",
       " 'puta': 2,\n",
       " 'excelente': 2,\n",
       " 'saud√°vel': 2,\n",
       " 'catarse': 2,\n",
       " 'ali√°s': 2,\n",
       " 'mesmo': 4,\n",
       " 'oxigene': 2,\n",
       " 'maoleskine': 2,\n",
       " 'maratonar': 2,\n",
       " 'breve': 2,\n",
       " 'again': 2,\n",
       " 'primeira': 2,\n",
       " 'lan√ßamento': 2,\n",
       " 'determina': 2,\n",
       " 'quanto': 4,\n",
       " 'cartaz': 2,\n",
       " 'essa': 5,\n",
       " 'lan√ßado': 2,\n",
       " 'maravilhoso': 2,\n",
       " 'mundo': 3,\n",
       " 's√©rio': 3,\n",
       " 'brasileiros': 2,\n",
       " '‚ù§': 2,\n",
       " 'Ô∏è': 3,\n",
       " 'sopranine': 1,\n",
       " 'ali': 3,\n",
       " 'juntas': 2,\n",
       " 'moskito': 2,\n",
       " 'recebendo': 2,\n",
       " 'mandar': 2,\n",
       " 'grupo': 2,\n",
       " 'dividido': 2,\n",
       " 'amou': 2,\n",
       " 'odiou': 2,\n",
       " 'idosas': 2,\n",
       " 'estavam': 2,\n",
       " 'gostaram': 3,\n",
       " 'admitiram': 2,\n",
       " 'bem': 3,\n",
       " 'feito': 1,\n",
       " 'wotzik': 2,\n",
       " 'brasil': 3,\n",
       " 'urgente': 2,\n",
       " 'vivam': 2,\n",
       " 'necess√°rio': 2,\n",
       " 'iskindolele': 2,\n",
       " 'estava': 3,\n",
       " 'obcecado': 3,\n",
       " 'passava': 3,\n",
       " '80': 3,\n",
       " 'outros': 3,\n",
       " 'torcia': 3,\n",
       " 'falasse': 3,\n",
       " 'sobre‚Ä¶': 1,\n",
       " 'por√©m': 3,\n",
       " 'afirmo': 2,\n",
       " 'tranquilidade': 2,\n",
       " 'milenio': 2,\n",
       " 'veja': 3,\n",
       " 'molena': 2,\n",
       " 'incentive': 2,\n",
       " 'nacional': 4,\n",
       " 'meter': 2,\n",
       " 'doido': 2,\n",
       " 'logo': 2,\n",
       " 'iguatemi': 2,\n",
       " 'antes': 3,\n",
       " 'fique': 2,\n",
       " 'sem': 3,\n",
       " 'minh': 2,\n",
       " 'dando': 2,\n",
       " 'palestra': 2,\n",
       " 'pessoal': 2,\n",
       " 'fez': 2,\n",
       " 'trilha': 2,\n",
       " 'sonora': 2,\n",
       " 'mimos': 2,\n",
       " 'pai': 2,\n",
       " 'aquelacristiana': 2,\n",
       " 'f√£s': 3,\n",
       " '=': 3,\n",
       " 'melhores': 3,\n",
       " 'parece': 2,\n",
       " 'infinitamente': 2,\n",
       " 'interessante': 2,\n",
       " 'rodrigoazo': 2,\n",
       " 'olha': 3,\n",
       " 'motivo': 2,\n",
       " 'vermos': 2,\n",
       " 'filma√ßo': 3,\n",
       " 'ode': 2,\n",
       " 'nosso': 2,\n",
       " 'sobretudo': 2,\n",
       " 'valente': 2,\n",
       " 'povo': 2,\n",
       " 'nordestino': 2,\n",
       " 'sensa√ß√£o': 2,\n",
       " 'reencontro': 2,\n",
       " 'cultura': 2,\n",
       " 'retrato': 2,\n",
       " 'hist√≥rico': 2,\n",
       " 'majestoso': 2,\n",
       " 'viva': 2,\n",
       " 'chegou': 2,\n",
       " 'jundia√≠': 2,\n",
       " 'sexo': 1,\n",
       " 'sagrado': 2,\n",
       " 'seu': 2,\n",
       " 'corpo': 2,\n",
       " 'templo': 2,\n",
       " 'compartilh√°': 2,\n",
       " 'louca': 2,\n",
       " 'sozinha': 2,\n",
       " 'msm': 1,\n",
       " 'drag√£o': 2,\n",
       " 'sigur_ross_': 2,\n",
       " 'arrasta': 2,\n",
       " 'maxxxramon': 2,\n",
       " 'choque': 2,\n",
       " 'tbt': 2,\n",
       " 'assista': 2,\n",
       " 'cr√≠tica': 3,\n",
       " 'arrobanerd': 2,\n",
       " 'longa': 2,\n",
       " 'traz': 2,\n",
       " 'reviravoltas': 2,\n",
       " 'tirar': 2,\n",
       " 'f√¥lego': 2,\n",
       " 'paulo__junior__': 1,\n",
       " 'ignorou': 2,\n",
       " 'som': 2,\n",
       " 'redor': 2,\n",
       " 'principal': 2,\n",
       " 'resistia': 2,\n",
       " 'exatamente': 2,\n",
       " 'rasteira': 2,\n",
       " 'contradit√≥rio': 2,\n",
       " 'nas': 3,\n",
       " 'ideias': 2,\n",
       " 'escreve': 2,\n",
       " 'saber': 2,\n",
       " 'met√°fora': 2,\n",
       " 'p': 3,\n",
       " 'gyn': 1,\n",
       " 't√¥': 1,\n",
       " 'lembrando': 2,\n",
       " 'cena': 2,\n",
       " 'fala': 2,\n",
       " 'personagens': 2,\n",
       " 'sob': 2,\n",
       " 'efeito': 2,\n",
       " 'forte': 2,\n",
       " 'psicotr√≥pico': 2,\n",
       " 'morrer': 3,\n",
       " 'hauahuahuahuhauauuahuahuauuahuahhuahuauuahuahhuahuauuahuah': 1,\n",
       " 'camisetinha': 1,\n",
       " 'temos': 2,\n",
       " 'so': 3,\n",
       " '1': 2,\n",
       " 'pessoa': 4,\n",
       " 'reclamando': 2,\n",
       " 'alguma': 3,\n",
       " 'boyzinha': 2,\n",
       " 'xuliaxx': 2,\n",
       " 'zap': 2,\n",
       " 'conversar': 2,\n",
       " 'seguinte': 2,\n",
       " 'eupalmeirensa': 2,\n",
       " 'laianexx': 2,\n",
       " 'laiane': 2,\n",
       " 'quer': 2,\n",
       " 'morenamoraes': 2,\n",
       " 'abruxapreta': 2,\n",
       " 'cinemarkoficial': 2,\n",
       " 'querendo': 2,\n",
       " 'salas': 2,\n",
       " 'posso': 2,\n",
       " 'abre': 2,\n",
       " 'nenhuma': 2,\n",
       " 'h4ckaq': 2,\n",
       " 'dor': 2,\n",
       " 'cabe√ßa': 2,\n",
       " 'enjoo': 2,\n",
       " 'vontade': 3,\n",
       " 'ressaca': 2,\n",
       " 'di√°logos': 2,\n",
       " 'companhia': 2,\n",
       " 'sincera': 2,\n",
       " 'tomar': 3,\n",
       " 'a√ßa√≠': 2,\n",
       " 'verdade': 2,\n",
       " 'beber': 2,\n",
       " 'aff': 1,\n",
       " 'sinopse': 2,\n",
       " 'interessou': 2,\n",
       " 'bastante': 2,\n",
       " 'lucasnunnes97': 2,\n",
       " '_msoliveira': 2,\n",
       " 'processando': 2,\n",
       " 'tanta': 1,\n",
       " 'atualmente': 2,\n",
       " 'gastei': 2,\n",
       " 'chance': 2,\n",
       " 'vendo': 2,\n",
       " 'aquela': 2,\n",
       " 'tranqueira': 2,\n",
       " 'incr√≠vel': 2,\n",
       " 'adjetivos': 2,\n",
       " 'qualidade': 2,\n",
       " 'deveriam': 2,\n",
       " 'provavelmente': 2,\n",
       " 'ja': 2,\n",
       " 'cinemark': 2,\n",
       " 'ent√£o': 2,\n",
       " 'cuiabano': 2,\n",
       " 'prestigiar': 2,\n",
       " 'hein': 1,\n",
       " 'faltava': 2,\n",
       " 'unisse': 2,\n",
       " 'todas': 2,\n",
       " 'tribos': 2,\n",
       " 'pablomoreno': 2,\n",
       " 'amei': 3,\n",
       " 'sert√£o': 2,\n",
       " 'impacto': 3,\n",
       " 'lucidez': 2,\n",
       " 'realiza√ß√£o': 2,\n",
       " 'precisa': 2,\n",
       " 'digerir': 2,\n",
       " 'ap√≥s': 3,\n",
       " 'muitos': 2,\n",
       " 'elogios': 2,\n",
       " 'espero': 2,\n",
       " 'encante': 2,\n",
       " 'hj': 2,\n",
       " 'assisto': 3,\n",
       " 'refrescos': 2,\n",
       " 'agenda': 2,\n",
       " 'conturbada': 1,\n",
       " 'par√ßa': 2,\n",
       " '‚ô•': 2,\n",
       " 'meio': 3,\n",
       " 'loucura': 2,\n",
       " 'assistido': 2,\n",
       " 'mandei': 2,\n",
       " 'tweet': 2,\n",
       " 'mauromendoncaf': 2,\n",
       " 'engravidar': 3,\n",
       " 'louco': 2,\n",
       " 'tesao': 2,\n",
       " 'sendo': 2,\n",
       " 'socorro': 2,\n",
       " 'dizer': 2,\n",
       " '‚Äúamiga': 2,\n",
       " 'juntas‚Äù': 2,\n",
       " 'supero': 1,\n",
       " 'aniversario': 2,\n",
       " 'paga': 2,\n",
       " '√∫nica': 2,\n",
       " 'entendi': 2,\n",
       " 'ns': 1,\n",
       " 'this': 2,\n",
       " 'is': 2,\n",
       " 'homofobia': 2,\n",
       " 'filhudi': 2,\n",
       " 'filhusi': 2,\n",
       " 'brasilia': 2,\n",
       " 'w': 1,\n",
       " 'f√©': 2,\n",
       " 'poder': 3,\n",
       " 'fernanda': 1,\n",
       " 'reservando': 2,\n",
       " 's√£o': 2,\n",
       " 'luiz': 2,\n",
       " 'alunos': 2,\n",
       " 'ü§ß': 2,\n",
       " 'ü•∞': 2,\n",
       " 'üò≠': 2,\n",
       " '√≥bvio': 2,\n",
       " 'enfiar': 2,\n",
       " 'obra': 2,\n",
       " 'inspira': 2,\n",
       " 'maravilha': 2,\n",
       " 'silveropereira': 2,\n",
       " 'votei': 2,\n",
       " 'assistiria': 2,\n",
       " 'hora': 2,\n",
       " 'penso': 3,\n",
       " 'perplexo': 1,\n",
       " 'recorte': 2,\n",
       " 'irregular': 2,\n",
       " 'obsess√µes': 2,\n",
       " 'exporta√ß√£o': 2,\n",
       " 'comentar': 3,\n",
       " 'realidade': 2,\n",
       " 'amigas': 1,\n",
       " 'essas': 2,\n",
       " 'hist√≥rias': 2,\n",
       " 'itimalian': 1,\n",
       " 'bacurit': 2,\n",
       " 'kkkkk': 2,\n",
       " 'aaa': 2,\n",
       " 'filha': 2,\n",
       " 'pois': 2,\n",
       " 'trate': 2,\n",
       " 'cinem': 2,\n",
       " 'volta': 3,\n",
       " 'agoraaaaaaa': 2,\n",
       " 'viaverdeshop': 1,\n",
       " 'amiguinho': 2,\n",
       " 'queira': 2,\n",
       " 'prefer√™ncia': 2,\n",
       " 'segunda': 2,\n",
       " 'quarta': 2,\n",
       " 'pre√ßo': 2,\n",
       " 'promocional': 2,\n",
       " 'postando': 2,\n",
       " 'plena': 2,\n",
       " 'convic√ß√£o': 2,\n",
       " 'flop': 2,\n",
       " 'citar': 2,\n",
       " 'correntes': 2,\n",
       " 'al√©m': 2,\n",
       " 'central': 2,\n",
       " 'ela': 2,\n",
       " 'amanh√£': 2,\n",
       " 'mesa': 2,\n",
       " 'buteco': 2,\n",
       " 'centro': 2,\n",
       " 'cmggg': 1,\n",
       " 'algm': 2,\n",
       " 'hd5trange': 2,\n",
       " 'nice': 1,\n",
       " 'comprando': 2,\n",
       " 'ingresso': 2,\n",
       " 'naquelas': 2,\n",
       " 'm√°quinas': 2,\n",
       " 'deu': 2,\n",
       " 'gritar': 2,\n",
       " 'film√£o': 2,\n",
       " 'caray': 2,\n",
       " 'onde': 2,\n",
       " 'refletir': 2,\n",
       " 'vir√°': 2,\n",
       " 'escolas': 2,\n",
       " 'museus': 2,\n",
       " 'sentido': 2,\n",
       " 'literal': 2,\n",
       " 'figurado': 2,\n",
       " 'tentar': 2,\n",
       " 'recuperar': 2,\n",
       " 'projeto': 2,\n",
       " 'entregar': 2,\n",
       " 'rever': 2,\n",
       " 'dressasantiago_': 2,\n",
       " 'diferenciado': 2,\n",
       " 'chamaram': 2,\n",
       " 'estamos': 2,\n",
       " 'evoluindo': 2,\n",
       " 'caionare4l': 2,\n",
       " 'vim': 2,\n",
       " 'humildemente': 2,\n",
       " 'irem': 2,\n",
       " 'assitir': 2,\n",
       " 'perfeitooo': 2,\n",
       " 'iamplanett': 1,\n",
       " 'aten√ß√£o': 1,\n",
       " '22': 1,\n",
       " '09': 1,\n",
       " 'passar': 1,\n",
       " 'vit√≥ria': 1,\n",
       " 'op√ß√£o': 1,\n",
       " 'barata': 1,\n",
       " 'local‚Ä¶': 1,\n",
       " 'bacuraufilme': 1,\n",
       " 'fa√ßam': 1,\n",
       " 'teste': 1,\n",
       " 'domingas': 1,\n",
       " 'orgulho': 1,\n",
       " 'objetivos': 1,\n",
       " 'cumpridos': 1,\n",
       " 'beareclama': 1,\n",
       " 'kkkk': 1,\n",
       " 'falta': 1,\n",
       " 'pacote': 1,\n",
       " 'teresa': 1,\n",
       " 'pospunkcearense': 1,\n",
       " 'tabacaria': 1,\n",
       " 'imposs√≠vel': 1,\n",
       " 'inagaki': 1,\n",
       " 'virar': 1,\n",
       " 'sou': 1,\n",
       " 'via': 1,\n",
       " 'davirocha': 1,\n",
       " 'bluecoloredboy': 1,\n",
       " 'conte': 1,\n",
       " 'amo': 1,\n",
       " 'socorr': 1,\n",
       " 'kkkkkkk': 1,\n",
       " 'tirou': 1,\n",
       " 'pr√≥pria': 1,\n",
       " 'cidade': 1,\n",
       " 'amig√°vel': 1,\n",
       " 'pac√≠fica': 1,\n",
       " 'tentando': 1,\n",
       " 'atrapalhar': 1,\n",
       " 'sua': 1,\n",
       " 'obrigada': 1,\n",
       " 'pr√≥prias': 1,\n",
       " 'r√©deas': 1,\n",
       " 'destino': 1,\n",
       " 'continuar': 1,\n",
       " 'sobrevivendo': 1,\n",
       " 'omegamark_xii': 1,\n",
       " 'te': 1,\n",
       " 'agostinho': 1,\n",
       " 'carrara': 1,\n",
       " 'matar': 1,\n",
       " 'carara': 1,\n",
       " 'matem√°tica': 1,\n",
       " 'basica': 1,\n",
       " 'marque': 1,\n",
       " 'ami‚Ä¶': 1,\n",
       " 'ifavmarkten': 1,\n",
       " 'tive': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for palavra in lista_relevante:\n",
    "    freq_rel[palavra]+=1\n",
    "freq_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nossa': 5,\n",
       " 'eu': 63,\n",
       " 'desviei': 2,\n",
       " 'muito': 17,\n",
       " 'r√°pido': 2,\n",
       " 'de': 269,\n",
       " 'um': 46,\n",
       " 'spoiler': 5,\n",
       " 'bacurau': 285,\n",
       " 'me': 21,\n",
       " 'senti': 2,\n",
       " 'ninja': 2,\n",
       " 'rt': 17,\n",
       " 'avokdoido': 4,\n",
       " 'quem': 8,\n",
       " 'nao': 9,\n",
       " 'gostou': 5,\n",
       " '√©': 110,\n",
       " 'no': 40,\n",
       " 'm√≠nimo': 7,\n",
       " 'do': 50,\n",
       " 'pouco': 5,\n",
       " 'nazista': 4,\n",
       " 'sim': 5,\n",
       " 'vi': 4,\n",
       " 'ontem': 1,\n",
       " 'com': 38,\n",
       " 'a': 107,\n",
       " 'viviane_cardoso': 1,\n",
       " 'o': 74,\n",
       " 'filme': 28,\n",
       " 'fant√°stico': 1,\n",
       " 'vale': 2,\n",
       " 'pena': 3,\n",
       " 'ver': 31,\n",
       " 'se': 29,\n",
       " 'voc√™': 75,\n",
       " 'tiver': 1,\n",
       " 'mente': 1,\n",
       " 'aberta': 1,\n",
       " 'e': 107,\n",
       " 'qualquer': 3,\n",
       " 'coisa': 5,\n",
       " 'que': 78,\n",
       " 'fa√ßo': 1,\n",
       " 'vivi': 1,\n",
       " 'sempre': 14,\n",
       " 'bom': 5,\n",
       " 'companheira': 5,\n",
       " 'filmes': 2,\n",
       " 'vida': 23,\n",
       " 'lutas': 1,\n",
       " 'lt': 3,\n",
       " '3': 4,\n",
       " 'hoje': 8,\n",
       " 'finalmente': 2,\n",
       " 'vejo': 2,\n",
       " 'ser√°': 1,\n",
       " 'consigo': 2,\n",
       " 'chegar': 1,\n",
       " 'at√©': 7,\n",
       " 'sala': 13,\n",
       " 'livre': 1,\n",
       " 't√°': 11,\n",
       " 'dif√≠cil': 1,\n",
       " 'lzanin': 2,\n",
       " 'texto': 2,\n",
       " 'marcelo': 3,\n",
       " 'coelho': 2,\n",
       " 'chama': 2,\n",
       " '‚Äòbolsonarisno': 2,\n",
       " 'sinal': 2,\n",
       " 'trocado‚Äô': 2,\n",
       " 'lembra': 2,\n",
       " 'ideia': 2,\n",
       " 'alguns': 2,\n",
       " 'jornalistas': 2,\n",
       " 'na': 24,\n",
       " '√©‚Ä¶': 1,\n",
       " 'diretor': 4,\n",
       " 'jo√£o': 2,\n",
       " 'kleber': 2,\n",
       " 'mendon√ßa': 3,\n",
       " 'filho': 3,\n",
       " 'n': 8,\n",
       " 'vou': 13,\n",
       " 'conseguir': 2,\n",
       " 'qual': 36,\n",
       " 'desse': 3,\n",
       " 'q': 11,\n",
       " 'galera': 4,\n",
       " 'ta': 5,\n",
       " 'doida': 4,\n",
       " 'achei': 2,\n",
       " 'era': 4,\n",
       " 'peixe': 2,\n",
       " 'isso': 7,\n",
       " 'vai': 10,\n",
       " 'assistir': 31,\n",
       " 'sai': 2,\n",
       " 'maluco': 2,\n",
       " 'todo': 10,\n",
       " 'dia': 5,\n",
       " 'bora': 1,\n",
       " 'galeres': 1,\n",
       " 'aproveitando': 1,\n",
       " 'pra': 30,\n",
       " 'divulgar': 1,\n",
       " 'melhor': 3,\n",
       " '2019': 3,\n",
       " 'personagem': 32,\n",
       " 'tirei': 75,\n",
       " 'lunga': 35,\n",
       " 'a√≠': 4,\n",
       " 'cara': 3,\n",
       " 'j√°': 25,\n",
       " 'assistiu': 3,\n",
       " 'ùêâùêöÃÅ': 2,\n",
       " 'ùêöùê¨ùê¨ùê¢ùê¨ùê≠ùê¢ùêÆ': 2,\n",
       " 'ùêÅùêöùêúùêÆùê´ùêöùêÆ': 2,\n",
       " 'jŒ¨': 2,\n",
       " 'Œ±ssŒπsœÑŒπœÖ': 2,\n",
       " 'Œ≤Œ±cœÖrŒ±œÖ': 2,\n",
       " 'ùïµùñÜÃÅ': 2,\n",
       " 'ùñÜùñòùñòùñéùñòùñôùñéùñö': 2,\n",
       " 'ùï≠ùñÜùñàùñöùñóùñÜùñö': 2,\n",
       " 'j·¥ÄÃÅ': 2,\n",
       " '·¥Äss…™s·¥õ…™·¥ú': 2,\n",
       " 'b·¥Ä·¥Ñ·¥ú Ä·¥Ä·¥úÔπñ': 2,\n",
       " 'foda': 2,\n",
       " 'omelete': 2,\n",
       " 'ainda': 17,\n",
       " 'foi': 8,\n",
       " 'visto': 3,\n",
       " 'por': 17,\n",
       " 'mais': 15,\n",
       " '130': 2,\n",
       " 'mil': 2,\n",
       " 'pessoas': 2,\n",
       " 'diariosm': 1,\n",
       " 'sucesso': 1,\n",
       " 'p√∫blico': 1,\n",
       " 'n√£o': 34,\n",
       " 'deve': 3,\n",
       " 'ser': 9,\n",
       " 'exibido': 1,\n",
       " 'nos': 8,\n",
       " 'cinemas': 4,\n",
       " 'santa': 1,\n",
       " 'maria': 1,\n",
       " 'aparentemente': 1,\n",
       " 's√≥': 7,\n",
       " 'ao': 13,\n",
       " 'nesse': 3,\n",
       " 'pa√≠s': 1,\n",
       " 's√°bado': 1,\n",
       " 'to': 6,\n",
       " 'ansios√≠ssimaaaa': 1,\n",
       " 'fds': 1,\n",
       " 'passa': 5,\n",
       " 'saindo': 1,\n",
       " 'cedo': 2,\n",
       " 'da': 37,\n",
       " 'faculdade': 1,\n",
       " 'correndo': 1,\n",
       " 'kmendoncafilho': 19,\n",
       " 'cine': 17,\n",
       " 'arte': 12,\n",
       " 'uff': 12,\n",
       " 'uma': 42,\n",
       " 'equipada': 12,\n",
       " 'como': 16,\n",
       " 'poucas': 12,\n",
       " 'numa': 13,\n",
       " 'universidade': 12,\n",
       " 'p√∫blica': 12,\n",
       " 'lotada': 12,\n",
       " 'estudantes': 12,\n",
       " 'para': 48,\n",
       " 'brasi‚Ä¶': 1,\n",
       " 'fazendo': 2,\n",
       " 'propaganda': 1,\n",
       " 'grandao': 1,\n",
       " 'pro': 1,\n",
       " 'uber': 2,\n",
       " 'aqui': 10,\n",
       " 'ele': 1,\n",
       " 'disse': 3,\n",
       " 'domingo': 1,\n",
       " 'perfeito': 1,\n",
       " 'poxa': 1,\n",
       " 'algu√©m': 6,\n",
       " 'leva': 2,\n",
       " 'psousadealmeida': 1,\n",
       " 'tenho': 3,\n",
       " 'novo': 15,\n",
       " 'tarantino': 4,\n",
       " 'it': 1,\n",
       " 'aaaaaa': 2,\n",
       " 'favor': 3,\n",
       " 'parem': 4,\n",
       " 'falar': 16,\n",
       " '√†s': 4,\n",
       " '20': 5,\n",
       " '40': 2,\n",
       " 'chamem': 1,\n",
       " 'sabe': 10,\n",
       " 'bacurinha': 3,\n",
       " 'thalitamari': 2,\n",
       " '_meirabeatriz': 2,\n",
       " 'menina': 2,\n",
       " 'agora': 8,\n",
       " 'tu': 9,\n",
       " 'tava': 4,\n",
       " 'falando': 6,\n",
       " 'kkkkkk': 2,\n",
       " 'apois': 2,\n",
       " 'gente': 33,\n",
       " 'faz': 4,\n",
       " 'nem': 5,\n",
       " 'treinamento': 2,\n",
       " 'pq': 7,\n",
       " 'esse': 9,\n",
       " 'teria': 2,\n",
       " 'oposi√ß√£o': 1,\n",
       " 'queria': 4,\n",
       " 'ter': 9,\n",
       " 'dinheiro': 7,\n",
       " 'wwwmlna': 6,\n",
       " 'algum': 8,\n",
       " 'homem': 7,\n",
       " 'feminista': 5,\n",
       " 'online': 6,\n",
       " 'disposto': 5,\n",
       " 'dar': 11,\n",
       " 'professora': 1,\n",
       " 'didatica': 2,\n",
       " 'perguntando': 7,\n",
       " 'turma': 4,\n",
       " 'viu': 8,\n",
       " 'mas': 26,\n",
       " 'voc√™s': 2,\n",
       " 'foram': 3,\n",
       " 'gostei': 1,\n",
       " 'tiduraes': 2,\n",
       " 'em': 32,\n",
       " '31¬∞': 2,\n",
       " 'top': 4,\n",
       " '250': 2,\n",
       " 'letterboxd': 3,\n",
       " 'lindo': 2,\n",
       " 'comigo': 4,\n",
       " 'anarcofino': 5,\n",
       " 'minha': 14,\n",
       " 'ex': 5,\n",
       " 'cineasta': 5,\n",
       " 'cheia': 5,\n",
       " 'dos': 6,\n",
       " 'contatos': 5,\n",
       " 'manda': 5,\n",
       " 'mensagem': 6,\n",
       " 'digo': 6,\n",
       " 'com‚Ä¶': 3,\n",
       " 'hortawitch': 1,\n",
       " 'assistindo': 2,\n",
       " 'estou': 8,\n",
       " 'vivendo': 1,\n",
       " 'ou': 13,\n",
       " 'apenas': 2,\n",
       " 'curtindo': 1,\n",
       " 'retwittando': 1,\n",
       " 'repostando': 1,\n",
       " 'coisas': 2,\n",
       " 'sobre': 9,\n",
       " 'baruchinha': 1,\n",
       " 'tem': 32,\n",
       " 'acho': 3,\n",
       " 'amiga': 3,\n",
       " 'pfvr': 1,\n",
       " '2': 3,\n",
       " 'pode': 5,\n",
       " 'ganhar': 1,\n",
       " 'pr√™mio': 3,\n",
       " 'ano': 2,\n",
       " 'beijos': 2,\n",
       " 'sabia': 1,\n",
       " 'karine': 1,\n",
       " 'teles': 1,\n",
       " 'dona': 2,\n",
       " 'meu': 17,\n",
       " 'c': 1,\n",
       " 'susto': 1,\n",
       " 'thread': 1,\n",
       " 'deliciosa': 1,\n",
       " 'tes√£o': 1,\n",
       " 'estreia': 4,\n",
       " '10de10': 5,\n",
       " 'caazalberto': 3,\n",
       " 'escreveu': 3,\n",
       " 'momento': 4,\n",
       " 'boca': 4,\n",
       " 'garotada': 3,\n",
       " 'incomodando': 1,\n",
       " 'gado': 1,\n",
       " 'üî•': 1,\n",
       " 'üêÇ': 1,\n",
       " 'review': 2,\n",
       " 'completo': 1,\n",
       " 'site': 1,\n",
       " 'üëâ': 1,\n",
       " 'affandre': 1,\n",
       " 'tanto': 3,\n",
       " 'invis√≠vel': 1,\n",
       " 'ser√£o': 1,\n",
       " 'exibidos': 1,\n",
       " 'festival': 2,\n",
       " 'toronto': 1,\n",
       " 'grandes': 1,\n",
       " 've√≠culos': 1,\n",
       " 'temporada': 1,\n",
       " 'premia√ß‚Ä¶': 1,\n",
       " 'aconteceu': 1,\n",
       " 'tudo': 25,\n",
       " 'ruim': 1,\n",
       " 'clap': 1,\n",
       " 'existem': 1,\n",
       " 'imperd√≠veis': 1,\n",
       " 'pela': 2,\n",
       " 'relev√¢ncia': 1,\n",
       " 'tem√°tica': 1,\n",
       " 'admir√°veis': 1,\n",
       " 'pelo': 7,\n",
       " 'significado': 2,\n",
       " 'cultural': 1,\n",
       " 'memor√°veis': 1,\n",
       " 'serem': 1,\n",
       " 'emblem√°ticos': 1,\n",
       " 'tendo': 1,\n",
       " 'monitoria': 2,\n",
       " 'exposi√ß√£o': 2,\n",
       " 'gringos': 2,\n",
       " 'depois': 12,\n",
       " 'matei': 2,\n",
       " 'todos': 12,\n",
       " 'sujei': 2,\n",
       " 'os': 8,\n",
       " 'quadros': 2,\n",
       " 'sangue': 1,\n",
       " 'tempos': 1,\n",
       " 'sombrios': 1,\n",
       " 'apreciar': 1,\n",
       " 'boa': 13,\n",
       " 'resist√™ncia': 1,\n",
       " '07': 1,\n",
       " 'setembro': 5,\n",
       " 'po√ßos': 1,\n",
       " 'caldas': 1,\n",
       " 'estarei': 1,\n",
       " 'l√°': 3,\n",
       " 'ingressos': 1,\n",
       " 'comprados': 1,\n",
       " 'quero': 4,\n",
       " 'mto': 1,\n",
       " 'ir': 8,\n",
       " 'caralhoo': 1,\n",
       " 'preciso': 1,\n",
       " 've': 1,\n",
       " 'namoral': 1,\n",
       " 'assistam': 2,\n",
       " 'pqp': 1,\n",
       " 'temia': 1,\n",
       " 'veio': 1,\n",
       " 'cin√©filo': 1,\n",
       " 'perto': 1,\n",
       " 'mim': 5,\n",
       " 't√£o': 2,\n",
       " 'more': 1,\n",
       " 'bater': 13,\n",
       " 'palma': 1,\n",
       " 'tailand√™s': 1,\n",
       " 'for': 3,\n",
       " 'va': 2,\n",
       " 'paz': 6,\n",
       " 'casa': 6,\n",
       " 'caralho': 1,\n",
       " 'fica': 1,\n",
       " 'come√ßa': 1,\n",
       " 'pensar': 2,\n",
       " 'cinema': 16,\n",
       " 'cada': 1,\n",
       " 'pensada': 1,\n",
       " 'd√°': 1,\n",
       " 'baixar': 1,\n",
       " 'minhas': 1,\n",
       " 'expectativas': 1,\n",
       " 'porque': 2,\n",
       " 'gosto': 1,\n",
       " 'expectativa': 1,\n",
       " 'alta': 2,\n",
       " 'assim': 2,\n",
       " 'ajuda': 4,\n",
       " 'tempo': 2,\n",
       " 'louvando': 1,\n",
       " 'vcs': 3,\n",
       " 'ficam': 2,\n",
       " 'ai': 9,\n",
       " 'oxewill94': 2,\n",
       " 'vez': 1,\n",
       " 'bicho': 1,\n",
       " 'avengers': 1,\n",
       " 'adoro': 1,\n",
       " 'consumo': 1,\n",
       " 'brasileiro': 4,\n",
       " 'saco': 15,\n",
       " 'cheio': 14,\n",
       " 'pararem': 1,\n",
       " 'fico': 2,\n",
       " 'quando': 7,\n",
       " 'falam': 1,\n",
       " 'insistentemente': 1,\n",
       " 'outra': 3,\n",
       " 'diegoquaglia2': 2,\n",
       " 'sei': 8,\n",
       " 'odeio': 3,\n",
       " 'black': 3,\n",
       " 'mirror': 2,\n",
       " 'compara√ß√£o': 2,\n",
       " '‚Ä¶': 2,\n",
       " 'constrangedor': 2,\n",
       " 'existir': 2,\n",
       " 'instagram': 2,\n",
       " 'chamado': 3,\n",
       " 'bacurau_memes': 2,\n",
       " 'deus': 3,\n",
       " 'fa√ßa': 1,\n",
       " 'cair': 1,\n",
       " 'tempi': 1,\n",
       " 'ain': 1,\n",
       " 'aguento': 3,\n",
       " 'ouvir': 3,\n",
       " 'esses': 3,\n",
       " 'porra': 3,\n",
       " 'entulhando': 1,\n",
       " 'timeline': 1,\n",
       " 'her√≥i': 1,\n",
       " 'hollywood': 1,\n",
       " 'ok': 1,\n",
       " 'falo': 1,\n",
       " 'nada': 4,\n",
       " 'deixa': 1,\n",
       " 'exaltar': 1,\n",
       " 'daqui': 3,\n",
       " 'pouquinho': 1,\n",
       " 'tb': 1,\n",
       " 'fazer': 4,\n",
       " 'meus': 2,\n",
       " 'amigos': 1,\n",
       " 'assistirem': 1,\n",
       " 'v√£o': 3,\n",
       " 'vamos': 3,\n",
       " 'la': 1,\n",
       " 'mudar': 1,\n",
       " 'pais': 1,\n",
       " 'eh': 4,\n",
       " 'possivel': 1,\n",
       " 'eles': 8,\n",
       " 'tenham': 1,\n",
       " 'realmente': 1,\n",
       " 'tirado': 1,\n",
       " 'edir': 2,\n",
       " 'macedo': 1,\n",
       " 'passando': 3,\n",
       " '50': 1,\n",
       " 'semana': 13,\n",
       " 'dias_ligia': 1,\n",
       " 'ningu√©m': 2,\n",
       " 'perguntou': 1,\n",
       " 'ameei': 1,\n",
       " 'amor': 2,\n",
       " 'bolha': 1,\n",
       " 'hype': 1,\n",
       " 'est√°': 14,\n",
       " 'maior': 1,\n",
       " 'cmg': 1,\n",
       " 'sido': 1,\n",
       " 'lana': 1,\n",
       " 'del': 1,\n",
       " 'rey': 1,\n",
       " 'harry': 1,\n",
       " 'styles': 1,\n",
       " 'errado': 3,\n",
       " 'nenhum': 1,\n",
       " 'deles': 1,\n",
       " 'desisti': 1,\n",
       " 'isto': 15,\n",
       " 'capitulo': 1,\n",
       " 'ova': 1,\n",
       " 'apoiar': 2,\n",
       " 'esganado': 1,\n",
       " 'pelos': 1,\n",
       " 'norte': 4,\n",
       " 'americanos': 5,\n",
       " 'kkkkkkkk': 2,\n",
       " 'disseram': 1,\n",
       " 'assisti': 6,\n",
       " 'lo': 1,\n",
       " 'vem': 6,\n",
       " 'entrar': 1,\n",
       " 'internet': 1,\n",
       " 'alheia': 1,\n",
       " 'outro': 1,\n",
       " 'tbm': 1,\n",
       " 'aquarius': 1,\n",
       " 'renatoxavoso': 3,\n",
       " 'kl√©ber': 2,\n",
       " 'entre': 3,\n",
       " 'coma': 2,\n",
       " 'cu': 2,\n",
       " 'toda': 4,\n",
       " 'fam√≠lia': 4,\n",
       " 'obrigado': 5,\n",
       " 'demais': 4,\n",
       " 'inuyalice': 1,\n",
       " 'fui': 3,\n",
       " '99': 2,\n",
       " 'certeza': 2,\n",
       " 'ia': 2,\n",
       " 'sair': 4,\n",
       " 'tiraram': 2,\n",
       " 'yesterday': 1,\n",
       " 'amigo': 3,\n",
       " 'sess√£o': 6,\n",
       " 'das': 5,\n",
       " '9': 2,\n",
       " 'teve': 1,\n",
       " 'alguem': 2,\n",
       " 'amanha': 1,\n",
       " 'stargirllv': 1,\n",
       " 'carai': 3,\n",
       " 'b√™': 1,\n",
       " 'indo': 1,\n",
       " 'yuri_reinaldo': 1,\n",
       " '16': 2,\n",
       " 'horas': 3,\n",
       " 'pensando': 2,\n",
       " 'cinesiageek': 1,\n",
       " 'deveria': 15,\n",
       " 'elogio': 1,\n",
       " 'tipo': 2,\n",
       " 'm√≥': 1,\n",
       " 'com√©dia': 1,\n",
       " 'a√ß√£o': 1,\n",
       " 'drama': 2,\n",
       " 'it2': 1,\n",
       " 'mt': 1,\n",
       " 'ansiosa': 1,\n",
       " 'principalmente': 1,\n",
       " 'as': 18,\n",
       " 'meninas': 1,\n",
       " 'ü•≥': 1,\n",
       " 'aula': 2,\n",
       " 'vsffffffff': 1,\n",
       " 'lugar': 1,\n",
       " 'pros': 1,\n",
       " 'lados': 1,\n",
       " 'zona': 1,\n",
       " 'madureira': 1,\n",
       " 'shopping': 1,\n",
       " 'rei': 1,\n",
       " 'leao': 1,\n",
       " 'programado': 1,\n",
       " 'meier': 1,\n",
       " 'sessao': 1,\n",
       " '21h': 1,\n",
       " 'noite': 3,\n",
       " 'puta': 3,\n",
       " 'excelente': 2,\n",
       " 'saud√°vel': 1,\n",
       " 'catarse': 1,\n",
       " 'ali√°s': 12,\n",
       " 'mesmo': 3,\n",
       " 'oxigene': 1,\n",
       " 'maoleskine': 1,\n",
       " 'maratonar': 1,\n",
       " 'breve': 1,\n",
       " 'again': 1,\n",
       " 'primeira': 1,\n",
       " 'lan√ßamento': 1,\n",
       " 'determina': 1,\n",
       " 'quanto': 1,\n",
       " 'cartaz': 20,\n",
       " 'essa': 3,\n",
       " 'lan√ßado': 1,\n",
       " 'maravilhoso': 1,\n",
       " 'mundo': 9,\n",
       " 's√©rio': 1,\n",
       " 'brasileiros': 1,\n",
       " '‚ù§': 1,\n",
       " 'Ô∏è': 4,\n",
       " 'sopranine': 1,\n",
       " 'ali': 1,\n",
       " 'juntas': 1,\n",
       " 'moskito': 1,\n",
       " 'recebendo': 1,\n",
       " 'mandar': 1,\n",
       " 'grupo': 1,\n",
       " 'dividido': 1,\n",
       " 'amou': 1,\n",
       " 'odiou': 1,\n",
       " 'idosas': 1,\n",
       " 'estavam': 1,\n",
       " 'gostaram': 1,\n",
       " 'admitiram': 1,\n",
       " 'bem': 15,\n",
       " 'feito': 1,\n",
       " 'wotzik': 1,\n",
       " 'brasil': 9,\n",
       " 'urgente': 1,\n",
       " 'vivam': 1,\n",
       " 'necess√°rio': 1,\n",
       " 'iskindolele': 1,\n",
       " 'estava': 2,\n",
       " 'obcecado': 1,\n",
       " 'passava': 1,\n",
       " '80': 1,\n",
       " 'outros': 1,\n",
       " 'torcia': 1,\n",
       " 'falasse': 1,\n",
       " 'sobre‚Ä¶': 1,\n",
       " 'por√©m': 1,\n",
       " 'afirmo': 1,\n",
       " 'tranquilidade': 1,\n",
       " 'milenio': 1,\n",
       " 'veja': 7,\n",
       " 'molena': 1,\n",
       " 'incentive': 1,\n",
       " 'nacional': 2,\n",
       " 'meter': 1,\n",
       " 'doido': 1,\n",
       " 'logo': 1,\n",
       " 'iguatemi': 1,\n",
       " 'antes': 8,\n",
       " 'fique': 1,\n",
       " 'sem': 1,\n",
       " 'minh': 1,\n",
       " 'dando': 1,\n",
       " 'palestra': 1,\n",
       " 'pessoal': 1,\n",
       " 'fez': 4,\n",
       " 'trilha': 8,\n",
       " 'sonora': 7,\n",
       " 'mimos': 1,\n",
       " 'pai': 2,\n",
       " 'aquelacristiana': 1,\n",
       " 'f√£s': 1,\n",
       " '=': 1,\n",
       " 'melhores': 2,\n",
       " 'parece': 3,\n",
       " 'infinitamente': 1,\n",
       " 'interessante': 1,\n",
       " 'rodrigoazo': 1,\n",
       " 'olha': 1,\n",
       " 'motivo': 1,\n",
       " 'vermos': 1,\n",
       " 'filma√ßo': 1,\n",
       " 'ode': 1,\n",
       " 'nosso': 2,\n",
       " 'sobretudo': 1,\n",
       " 'valente': 1,\n",
       " 'povo': 2,\n",
       " 'nordestino': 2,\n",
       " 'sensa√ß√£o': 1,\n",
       " 'reencontro': 1,\n",
       " 'cultura': 3,\n",
       " 'retrato': 1,\n",
       " 'hist√≥rico': 1,\n",
       " 'majestoso': 1,\n",
       " 'viva': 2,\n",
       " 'chegou': 2,\n",
       " 'jundia√≠': 1,\n",
       " 'sexo': 1,\n",
       " 'sagrado': 1,\n",
       " 'seu': 2,\n",
       " 'corpo': 1,\n",
       " 'templo': 1,\n",
       " 'compartilh√°': 1,\n",
       " 'louca': 1,\n",
       " 'sozinha': 1,\n",
       " 'msm': 1,\n",
       " 'drag√£o': 1,\n",
       " 'sigur_ross_': 1,\n",
       " 'arrasta': 1,\n",
       " 'maxxxramon': 2,\n",
       " 'choque': 1,\n",
       " 'tbt': 1,\n",
       " 'assista': 5,\n",
       " 'cr√≠tica': 3,\n",
       " 'arrobanerd': 1,\n",
       " 'longa': 1,\n",
       " 'traz': 1,\n",
       " 'reviravoltas': 1,\n",
       " 'tirar': 1,\n",
       " 'f√¥lego': 1,\n",
       " 'paulo__junior__': 12,\n",
       " 'ignorou': 1,\n",
       " 'som': 2,\n",
       " 'redor': 3,\n",
       " 'principal': 1,\n",
       " 'resistia': 1,\n",
       " 'exatamente': 1,\n",
       " 'rasteira': 1,\n",
       " 'contradit√≥rio': 1,\n",
       " 'nas': 1,\n",
       " 'ideias': 1,\n",
       " 'escreve': 1,\n",
       " 'saber': 1,\n",
       " 'met√°fora': 1,\n",
       " 'p': 4,\n",
       " 'gyn': 1,\n",
       " 't√¥': 20,\n",
       " 'lembrando': 1,\n",
       " 'cena': 1,\n",
       " 'fala': 1,\n",
       " 'personagens': 2,\n",
       " 'sob': 1,\n",
       " 'efeito': 1,\n",
       " 'forte': 8,\n",
       " 'psicotr√≥pico': 1,\n",
       " 'morrer': 2,\n",
       " 'hauahuahuahuhauauuahuahuauuahuahhuahuauuahuahhuahuauuahuah': 1,\n",
       " 'camisetinha': 1,\n",
       " 'temos': 1,\n",
       " 'so': 1,\n",
       " '1': 6,\n",
       " 'pessoa': 6,\n",
       " 'reclamando': 2,\n",
       " 'alguma': 1,\n",
       " 'boyzinha': 1,\n",
       " 'xuliaxx': 1,\n",
       " 'zap': 1,\n",
       " 'conversar': 1,\n",
       " 'seguinte': 1,\n",
       " 'eupalmeirensa': 2,\n",
       " 'laianexx': 1,\n",
       " 'laiane': 1,\n",
       " 'quer': 6,\n",
       " 'morenamoraes': 1,\n",
       " 'abruxapreta': 1,\n",
       " 'cinemarkoficial': 1,\n",
       " 'querendo': 1,\n",
       " 'salas': 2,\n",
       " 'posso': 1,\n",
       " 'abre': 1,\n",
       " 'nenhuma': 1,\n",
       " 'h4ckaq': 2,\n",
       " 'dor': 2,\n",
       " 'cabe√ßa': 2,\n",
       " 'enjoo': 1,\n",
       " 'vontade': 2,\n",
       " 'ressaca': 2,\n",
       " 'di√°logos': 1,\n",
       " 'companhia': 1,\n",
       " 'sincera': 1,\n",
       " 'tomar': 18,\n",
       " 'a√ßa√≠': 1,\n",
       " 'verdade': 1,\n",
       " 'beber': 1,\n",
       " 'aff': 5,\n",
       " 'sinopse': 1,\n",
       " 'interessou': 1,\n",
       " 'bastante': 1,\n",
       " 'lucasnunnes97': 1,\n",
       " '_msoliveira': 1,\n",
       " 'processando': 1,\n",
       " 'tanta': 1,\n",
       " 'atualmente': 1,\n",
       " 'gastei': 1,\n",
       " 'chance': 1,\n",
       " 'vendo': 2,\n",
       " 'aquela': 1,\n",
       " 'tranqueira': 1,\n",
       " 'incr√≠vel': 1,\n",
       " 'adjetivos': 1,\n",
       " 'qualidade': 2,\n",
       " 'deveriam': 1,\n",
       " 'provavelmente': 2,\n",
       " 'ja': 3,\n",
       " 'cinemark': 2,\n",
       " 'ent√£o': 4,\n",
       " 'cuiabano': 1,\n",
       " 'prestigiar': 1,\n",
       " 'hein': 1,\n",
       " 'faltava': 1,\n",
       " 'unisse': 1,\n",
       " 'todas': 1,\n",
       " 'tribos': 1,\n",
       " 'pablomoreno': 1,\n",
       " 'amei': 7,\n",
       " 'sert√£o': 1,\n",
       " 'impacto': 1,\n",
       " 'lucidez': 1,\n",
       " 'realiza√ß√£o': 1,\n",
       " 'precisa': 5,\n",
       " 'digerir': 1,\n",
       " 'ap√≥s': 1,\n",
       " 'muitos': 1,\n",
       " 'elogios': 1,\n",
       " 'espero': 1,\n",
       " 'encante': 1,\n",
       " 'hj': 2,\n",
       " 'assisto': 1,\n",
       " 'refrescos': 1,\n",
       " 'agenda': 1,\n",
       " 'conturbada': 1,\n",
       " 'par√ßa': 2,\n",
       " '‚ô•': 1,\n",
       " 'meio': 1,\n",
       " 'loucura': 1,\n",
       " 'assistido': 1,\n",
       " 'mandei': 1,\n",
       " 'tweet': 2,\n",
       " 'mauromendoncaf': 1,\n",
       " 'engravidar': 1,\n",
       " 'louco': 1,\n",
       " 'tesao': 1,\n",
       " 'sendo': 2,\n",
       " 'socorro': 1,\n",
       " 'dizer': 1,\n",
       " '‚Äúamiga': 1,\n",
       " 'juntas‚Äù': 1,\n",
       " 'supero': 1,\n",
       " 'aniversario': 1,\n",
       " 'paga': 1,\n",
       " '√∫nica': 1,\n",
       " 'entendi': 1,\n",
       " 'ns': 2,\n",
       " 'this': 2,\n",
       " 'is': 2,\n",
       " 'homofobia': 2,\n",
       " 'filhudi': 1,\n",
       " 'filhusi': 1,\n",
       " 'brasilia': 1,\n",
       " 'w': 1,\n",
       " 'f√©': 1,\n",
       " 'poder': 1,\n",
       " 'fernanda': 1,\n",
       " 'reservando': 1,\n",
       " 's√£o': 8,\n",
       " 'luiz': 1,\n",
       " 'alunos': 1,\n",
       " 'ü§ß': 1,\n",
       " 'ü•∞': 1,\n",
       " 'üò≠': 1,\n",
       " '√≥bvio': 3,\n",
       " 'enfiar': 1,\n",
       " 'obra': 1,\n",
       " 'inspira': 1,\n",
       " 'maravilha': 1,\n",
       " 'silveropereira': 1,\n",
       " 'votei': 1,\n",
       " 'assistiria': 1,\n",
       " 'hora': 2,\n",
       " 'penso': 1,\n",
       " 'perplexo': 1,\n",
       " 'recorte': 2,\n",
       " 'irregular': 2,\n",
       " 'obsess√µes': 2,\n",
       " 'exporta√ß√£o': 2,\n",
       " 'comentar': 1,\n",
       " 'realidade': 1,\n",
       " 'amigas': 1,\n",
       " 'essas': 1,\n",
       " 'hist√≥rias': 1,\n",
       " 'itimalian': 1,\n",
       " 'bacurit': 1,\n",
       " 'kkkkk': 2,\n",
       " 'aaa': 1,\n",
       " 'filha': 4,\n",
       " 'pois': 2,\n",
       " 'trate': 1,\n",
       " 'cinem': 1,\n",
       " 'volta': 3,\n",
       " 'agoraaaaaaa': 1,\n",
       " 'viaverdeshop': 1,\n",
       " 'amiguinho': 1,\n",
       " 'queira': 1,\n",
       " 'prefer√™ncia': 1,\n",
       " 'segunda': 2,\n",
       " 'quarta': 4,\n",
       " 'pre√ßo': 1,\n",
       " 'promocional': 1,\n",
       " 'postando': 2,\n",
       " 'plena': 1,\n",
       " 'convic√ß√£o': 1,\n",
       " 'flop': 1,\n",
       " 'citar': 1,\n",
       " 'correntes': 1,\n",
       " 'al√©m': 2,\n",
       " 'central': 1,\n",
       " 'ela': 10,\n",
       " 'amanh√£': 3,\n",
       " 'mesa': 1,\n",
       " 'buteco': 1,\n",
       " 'centro': 1,\n",
       " 'cmggg': 1,\n",
       " 'algm': 1,\n",
       " 'hd5trange': 1,\n",
       " 'nice': 1,\n",
       " 'comprando': 1,\n",
       " 'ingresso': 4,\n",
       " 'naquelas': 1,\n",
       " 'm√°quinas': 1,\n",
       " 'deu': 2,\n",
       " 'gritar': 1,\n",
       " 'film√£o': 1,\n",
       " 'caray': 1,\n",
       " 'onde': 2,\n",
       " 'refletir': 1,\n",
       " 'vir√°': 1,\n",
       " 'escolas': 1,\n",
       " 'museus': 1,\n",
       " 'sentido': 1,\n",
       " 'literal': 1,\n",
       " 'figurado': 1,\n",
       " 'tentar': 1,\n",
       " 'recuperar': 1,\n",
       " 'projeto': 2,\n",
       " 'entregar': 1,\n",
       " 'rever': 1,\n",
       " 'dressasantiago_': 1,\n",
       " 'diferenciado': 1,\n",
       " 'chamaram': 1,\n",
       " 'estamos': 2,\n",
       " 'evoluindo': 1,\n",
       " 'caionare4l': 1,\n",
       " 'vim': 5,\n",
       " 'humildemente': 1,\n",
       " 'irem': 2,\n",
       " 'assitir': 1,\n",
       " 'perfeitooo': 1,\n",
       " 'iamplanett': 2,\n",
       " 'aten√ß√£o': 2,\n",
       " '22': 2,\n",
       " '09': 2,\n",
       " 'passar': 11,\n",
       " 'vit√≥ria': 3,\n",
       " 'op√ß√£o': 2,\n",
       " 'barata': 2,\n",
       " 'local‚Ä¶': 1,\n",
       " 'bacuraufilme': 13,\n",
       " 'fa√ßam': 2,\n",
       " 'teste': 4,\n",
       " 'domingas': 36,\n",
       " 'orgulho': 2,\n",
       " 'objetivos': 9,\n",
       " 'cumpridos': 2,\n",
       " 'beareclama': 1,\n",
       " 'kkkk': 2,\n",
       " 'falta': 2,\n",
       " 'pacote': 18,\n",
       " 'teresa': 17,\n",
       " 'pospunkcearense': 1,\n",
       " 'tabacaria': 2,\n",
       " 'imposs√≠vel': 2,\n",
       " 'inagaki': 15,\n",
       " 'virar': 15,\n",
       " 'sou': 15,\n",
       " 'via': 4,\n",
       " 'davirocha': 1,\n",
       " 'bluecoloredboy': 2,\n",
       " 'conte': 2,\n",
       " 'amo': 9,\n",
       " 'socorr': 1,\n",
       " 'kkkkkkk': 2,\n",
       " 'tirou': 9,\n",
       " 'pr√≥pria': 17,\n",
       " 'cidade': 18,\n",
       " 'amig√°vel': 11,\n",
       " 'pac√≠fica': 11,\n",
       " 'tentando': 11,\n",
       " 'atrapalhar': 11,\n",
       " 'sua': 20,\n",
       " 'obrigada': 11,\n",
       " 'pr√≥prias': 11,\n",
       " 'r√©deas': 11,\n",
       " 'destino': 11,\n",
       " 'continuar': 11,\n",
       " 'sobrevivendo': 11,\n",
       " 'omegamark_xii': 5,\n",
       " 'te': 7,\n",
       " 'agostinho': 9,\n",
       " 'carrara': 5,\n",
       " 'matar': 6,\n",
       " 'carara': 5,\n",
       " 'matem√°tica': 5,\n",
       " 'basica': 5,\n",
       " 'marque': 5,\n",
       " 'ami‚Ä¶': 1,\n",
       " 'ifavmarkten': 2,\n",
       " 'tive': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for palavra in lista_irrelevante:\n",
    "    freq_irr[palavra]+=1\n",
    "freq_irr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementa√ß√£o do Naive Bayes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso foi utilizado o teorema de Bayes:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "P(relev√¢ncia|palavra) = \\frac{P(palavra|relev√¢ncia)P(relev√¢ncia)}{P(palavra)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_irr = {}\n",
    "prob_rel = {}\n",
    "\n",
    "for palavra in palavras_totais:\n",
    "    prob_rel[palavra]= freq_rel[palavra]/(len(palavras_totais)+quantidade_relevancia[1])\n",
    "    prob_irr[palavra]= freq_irr[palavra]/(len(palavras_totais)+quantidade_relevancia[0])\n",
    "    \n",
    "\n",
    "#probabilidade da relev√¢ncia em rela√ß√£o √†s palavras do tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora vamos testar o classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>algu√©m me convida pra assistir bacurau amanh√£</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kmendoncafilho as duas sessoes de bacurau hj ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cinearteuff bacurau tem entranhas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n√£o √© poss√≠vel que bacurau j√° esteja dispon√≠ve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s√≥ vou ver bacurau domingo boe  alguma gay pod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Relev√¢ncia\n",
       "0     algu√©m me convida pra assistir bacurau amanh√£            1\n",
       "1   kmendoncafilho as duas sessoes de bacurau hj ...           1\n",
       "2               cinearteuff bacurau tem entranhas              1\n",
       "3  n√£o √© poss√≠vel que bacurau j√° esteja dispon√≠ve...           0\n",
       "4  s√≥ vou ver bacurau domingo boe  alguma gay pod...           0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste_df = pd.DataFrame()\n",
    "teste_df[\"Tweets\"] = teste_limpo\n",
    "teste_df[\"Relev√¢ncia\"] = teste[\"Relev√¢ncia\"]\n",
    "\n",
    "teste_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "      <th>Chutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>algu√©m me convida pra assistir bacurau amanh√£</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kmendoncafilho as duas sessoes de bacurau hj ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cinearteuff bacurau tem entranhas</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n√£o √© poss√≠vel que bacurau j√° esteja dispon√≠ve...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s√≥ vou ver bacurau domingo boe  alguma gay pod...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Relev√¢ncia  Chutes\n",
       "0     algu√©m me convida pra assistir bacurau amanh√£            1       0\n",
       "1   kmendoncafilho as duas sessoes de bacurau hj ...           1       0\n",
       "2               cinearteuff bacurau tem entranhas              1       0\n",
       "3  n√£o √© poss√≠vel que bacurau j√° esteja dispon√≠ve...           0       0\n",
       "4  s√≥ vou ver bacurau domingo boe  alguma gay pod...           0       0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chutes = [] #lista com chutes baseados nas probabilidades\n",
    "for tweet in teste_limpo:\n",
    "    pI = 1\n",
    "    pR = 1\n",
    "    palavras = tweet.split(\" \")\n",
    "    \n",
    "    for palavra in palavras:\n",
    "        if palavra in prob_rel:\n",
    "            pR*= prob_rel[palavra]\n",
    "        else:\n",
    "            pR *= 1/(len(palavras_totais)+quantidade_relevancia[1])\n",
    "        if palavra in prob_irr:\n",
    "            pI*= prob_irr[palavra]\n",
    "        else:\n",
    "            pI *= 1/(len(palavras_totais)+quantidade_relevancia[0])\n",
    "    \n",
    "    Prob_Rel = quantidade_relevancia[1]*pR\n",
    "    Prob_Irr = quantidade_relevancia[0]*pI\n",
    "    \n",
    "    if Prob_Rel>= Prob_Irr:\n",
    "        chutes.append(1)\n",
    "    else:\n",
    "        chutes.append(0)\n",
    "#     print (Prob_Irr)\n",
    "        \n",
    "\n",
    "teste_df[\"Chutes\"]=chutes\n",
    "teste_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_certo = teste_df.loc[(teste_df['Relev√¢ncia']==1)&(teste_df['Chutes']==1), ['Relev√¢ncia','Chutes']]\n",
    "rel_err = teste_df.loc[(teste_df['Relev√¢ncia']!=1)&(teste_df['Chutes']==1), ['Relev√¢ncia','Chutes']]\n",
    "irr_certo = teste_df.loc[(teste_df['Relev√¢ncia']==0)&(teste_df['Chutes']==0), ['Relev√¢ncia','Chutes']]\n",
    "irr_err = teste_df.loc[(teste_df['Relev√¢ncia']!=0)&(teste_df['Chutes']==0), ['Relev√¢ncia','Chutes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fizemos um *crosstab* para comparar os resultados dados com os resultados previstos pelo treinamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Treinamento</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Treinamento       0     1   All\n",
       "Classifica√ß√£o                  \n",
       "0              0.38  0.00  0.38\n",
       "1              0.58  0.04  0.62\n",
       "All            0.96  0.04  1.00"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_sudoku = pd.crosstab(teste_df[\"Relev√¢ncia\"], teste_df[\"Chutes\"], rownames=[\"Classifica√ß√£o\"], colnames=[\"Treinamento\"], margins=True, normalize=True)\n",
    "tabela_sudoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevantes corretos: 4.0%\n",
      "Irrelevantes corretos: 38.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Relevantes corretos: {0}%\".format((tabela_sudoku.loc[1,1])*100))\n",
    "print(\"Irrelevantes corretos: {0}%\".format((tabela_sudoku.loc[0,0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que nosso classificador marcou 4% dos *tweets* como relevantes quando eles realmente eram relevantes e 38% como irrelevantes corretamente. Esses valores representam que nosso classificador ainda precisa de ajustes e melhorias para que se torne mais preciso. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aperfei√ßoamento:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o Naive Bayes n√£o considera a correla√ß√£o entre as palavras, poderiamos fazer um programa que fizesse a combina√ß√£o entre elas para desenvolver um classificador ainda mais preciso. Por exemplo, poderiamos combinar as palavras \"n√£o\" e \"melhor\", para que o computador n√£o classifique erroneamente um *tweet* que esteja dizendo: \"Bacurau n√£o √© o melhor filme que eu j√° assisti\" como relevante, apenas porque apresenta a palavra \"melhor\" nele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  \n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) \n",
    "\n",
    "[Techniques for Improving the Performance\n",
    "of Naive Bayes for Text Classification](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.2085&rep=rep1&type=pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
