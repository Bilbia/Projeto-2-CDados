{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Beatriz Muniz de Castro e Silva\n",
    "\n",
    "Nome: Nicole Sarvasi Alves da Costa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "#Instalando o pacote emoji para limpar mensagens\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#importando os pacotes necess√°rios\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import functools\n",
    "import operator\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: @nicknennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @nickcnennis\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'bacurau'\n",
    "\n",
    "#Quantidade m√≠nima de mensagens capturadas:\n",
    "n = 700\n",
    "#Quantidade m√≠nima de mensagens para a base de treinamento:\n",
    "t = 500\n",
    "\n",
    "#Filtro de l√≠ngua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 401",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ef43b50e526d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#retira os retweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproduto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'extended'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretweeted\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'RT'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmsgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTweepError\u001b[0m: Twitter error response: status code = 401"
     ]
    }
   ],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documenta√ß√£o do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "\n",
    "#retira os retweets\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode='extended').items():  \n",
    "    if (not msg.retweeted) and ('RT' not in msg.full_text): \n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um poss√≠vel vi√©s\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo n√£o existe para n√£o substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa √© manual. Fa√ßa a mesma pelo Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leitura do arquivo excel\n",
    "treino = pd.read_excel(\"bacurau.xlsx\", \"Treinamento\") #tabela da parte de treinamento do excel\n",
    "teste = pd.read_excel(\"bacurau.xlsx\", \"Teste\") #tabela da parte de teste do excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    import string\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if not word.startswith('https'))\n",
    "    \n",
    "    text_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "    text_split_whitespace = [substr.split() for substr in text_split_emoji]\n",
    "    text_split = functools.reduce(operator.concat, text_split_whitespace)\n",
    "    text = ' '.join(word for word in text_split)\n",
    "    \n",
    "    punctuation = '[!-/.:?;@]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    nova_linha = '[\\n]'\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed = re.sub(nova_linha, \" \", text_subbed)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar uma lista com os tweets de teste limpos (sem @ e essas negocias loucas)\n",
    "teste_t = pd.Series(teste[\"Teste\"])\n",
    "teste_limpo = []\n",
    "for i in teste_t:\n",
    "    teste_limpo.append(cleanup(i.lower()))\n",
    "# teste_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar uma lista com os tweets de treinamento limpos (same as above)\n",
    "treino_s = pd.Series(treino[\"Treinamento\"])\n",
    "treino_limpo = []\n",
    "for i in treino_s:\n",
    "    treino_limpo.append(cleanup(str(i).lower()))\n",
    "# treino_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau       0.059806\n",
       "de            0.044272\n",
       "e             0.021100\n",
       "rt            0.021100\n",
       "a             0.019288\n",
       "√©             0.017864\n",
       "que           0.017735\n",
       "tirei         0.015534\n",
       "o             0.015146\n",
       "eu            0.012816\n",
       "voc√™          0.011133\n",
       "do            0.009968\n",
       "ver           0.009191\n",
       "pra           0.008932\n",
       "no            0.008026\n",
       "um            0.007896\n",
       "assistir      0.007896\n",
       "filme         0.007249\n",
       "n√£o           0.007120\n",
       "uma           0.006861\n",
       "com           0.006731\n",
       "para          0.006731\n",
       "em            0.006084\n",
       "da            0.005955\n",
       "tem           0.005696\n",
       "mas           0.005437\n",
       "gente         0.005437\n",
       "qual          0.004919\n",
       "domingas      0.004531\n",
       "lunga         0.004531\n",
       "                ...   \n",
       "tirar         0.000129\n",
       "esta          0.000129\n",
       "aquela        0.000129\n",
       "pensada       0.000129\n",
       "cad√™          0.000129\n",
       "molena        0.000129\n",
       "vivi          0.000129\n",
       "buteco        0.000129\n",
       "outro         0.000129\n",
       "cmg           0.000129\n",
       "ninja         0.000129\n",
       "grava√ß√µes     0.000129\n",
       "arrecada      0.000129\n",
       "temia         0.000129\n",
       "matastes      0.000129\n",
       "sorvete       0.000129\n",
       "desmerecer    0.000129\n",
       "10            0.000129\n",
       "√∫ltimos       0.000129\n",
       "super         0.000129\n",
       "shopping      0.000129\n",
       "escreve       0.000129\n",
       "derrotar      0.000129\n",
       "falacioso     0.000129\n",
       "deles         0.000129\n",
       "incentive     0.000129\n",
       "√≥dio          0.000129\n",
       "repostando    0.000129\n",
       "senhor        0.000129\n",
       "ova           0.000129\n",
       "Length: 1693, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino_t = (\" \").join(treino_limpo) #junta todos os tweets em uma string s√≥\n",
    "lista_relativa = treino_t.split() #divide a string pra fazer uma lista com cada palavra separadamente\n",
    "frequencia_absoluta = pd.Series(lista_relativa).value_counts()\n",
    "frequencia_relativa = pd.Series(lista_relativa).value_counts(True)\n",
    "frequencia_relativa #frequencia relativa total das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau            0.059778\n",
       "de                 0.034291\n",
       "que                0.027340\n",
       "e                  0.024096\n",
       "o                  0.018999\n",
       "pra                0.018536\n",
       "ver                0.018072\n",
       "a                  0.016682\n",
       "assistir           0.014365\n",
       "filme              0.013438\n",
       "√©                  0.012975\n",
       "do                 0.012975\n",
       "eu                 0.012512\n",
       "no                 0.010658\n",
       "n√£o                0.009268\n",
       "um                 0.007414\n",
       "em                 0.007414\n",
       "mas                0.006951\n",
       "cinema             0.006951\n",
       "com                0.006951\n",
       "mais               0.006487\n",
       "me                 0.006487\n",
       "tem                0.005561\n",
       "uma                0.005097\n",
       "sobre              0.005097\n",
       "bom                0.005097\n",
       "vou                0.005097\n",
       "da                 0.004634\n",
       "vai                0.004634\n",
       "q                  0.004634\n",
       "                     ...   \n",
       "existem            0.000463\n",
       "centro             0.000463\n",
       "sido               0.000463\n",
       "paga               0.000463\n",
       "novoalgu√©m         0.000463\n",
       "caionare4l         0.000463\n",
       "todas              0.000463\n",
       "resistia           0.000463\n",
       "maxxxramon         0.000463\n",
       "assistido          0.000463\n",
       "reservando         0.000463\n",
       "abruxapreta        0.000463\n",
       "elogios            0.000463\n",
       "aula               0.000463\n",
       "9alguem            0.000463\n",
       "companheira        0.000463\n",
       "ingresso           0.000463\n",
       "nenhuma            0.000463\n",
       "bora               0.000463\n",
       "maoleskine         0.000463\n",
       "psousadealmeida    0.000463\n",
       "ali√°s              0.000463\n",
       "emblem√°ticos       0.000463\n",
       "obra               0.000463\n",
       "americanos         0.000463\n",
       "verdade            0.000463\n",
       "deu                0.000463\n",
       "_msoliveira        0.000463\n",
       "rey                0.000463\n",
       "ova                0.000463\n",
       "Length: 878, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel = treino.index[treino.Relev√¢ncia == 1] #crit√©rio para selecionar s√≥ os tweets relevantes\n",
    "treino_r = treino.loc[rel, \"Treinamento\"]  #cria nova database com os tweets relevantes\n",
    "\n",
    "\n",
    "#frequ√™ncia das palavras nos tweets relevantes\n",
    "treino_rt = treino_r.str.cat()\n",
    "treino_rt = cleanup(treino_rt.lower())\n",
    "lista_relevante = treino_rt.split()\n",
    "frequencia_absoluta = pd.Series(lista_relevante).value_counts()\n",
    "frequencia_rel_relevantes = pd.Series(lista_relevante).value_counts(True)\n",
    "frequencia_rel_relevantes #frequ√™ncia relativa dos tweets relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau            0.054954\n",
       "de                 0.051858\n",
       "√©                  0.021091\n",
       "a                  0.020511\n",
       "e                  0.020511\n",
       "que                0.014899\n",
       "tirei              0.014319\n",
       "voc√™               0.014319\n",
       "o                  0.014125\n",
       "eu                 0.011997\n",
       "do                 0.009481\n",
       "para               0.009094\n",
       "um                 0.008707\n",
       "uma                0.007933\n",
       "no                 0.007546\n",
       "com                0.007159\n",
       "da                 0.006966\n",
       "domingas           0.006772\n",
       "qual               0.006772\n",
       "lunga              0.006579\n",
       "n√£o                0.006385\n",
       "gente              0.006192\n",
       "em                 0.005998\n",
       "personagem         0.005998\n",
       "tem                0.005998\n",
       "assistir           0.005805\n",
       "ver                0.005805\n",
       "pra                0.005611\n",
       "se                 0.005418\n",
       "filme              0.005224\n",
       "                     ...   \n",
       "coragem            0.000193\n",
       "rondelicia         0.000193\n",
       "tropa              0.000193\n",
       "cineart            0.000193\n",
       "√°gua               0.000193\n",
       "respons√°vel        0.000193\n",
       "resposta           0.000193\n",
       "delicia            0.000193\n",
       "juri               0.000193\n",
       "inventa            0.000193\n",
       "bacuraujusto       0.000193\n",
       "instagram          0.000193\n",
       "nomemedo           0.000193\n",
       "beijar             0.000193\n",
       "conseguindo        0.000193\n",
       "got                0.000193\n",
       "quadros            0.000193\n",
       "boazinha           0.000193\n",
       "brasi‚Ä¶por          0.000193\n",
       "comigort           0.000193\n",
       "gregoriorrr        0.000193\n",
       "certeza            0.000193\n",
       "kkkks√≥             0.000193\n",
       "vanity             0.000193\n",
       "mirror~~menina     0.000193\n",
       "ùïµùñÜÃÅ                0.000193\n",
       "cat√°rtica‚Ä¶carai    0.000193\n",
       "exporta√ß√£o         0.000193\n",
       "fa√ßam              0.000193\n",
       "kkkkk              0.000193\n",
       "Length: 1244, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irel = treino.index[treino.Relev√¢ncia == 0]\n",
    "treino_ir = treino.loc[irel, \"Treinamento\"]  #tweets irrelevantes\n",
    "\n",
    "#frequ√™ncia das palavras nos tweets irrelevantes\n",
    "treino_irt = treino_ir.str.cat()\n",
    "treino_irt = cleanup(treino_irt.lower())\n",
    "lista_irrelevante = treino_irt.split()\n",
    "frequencia_absoluta = pd.Series(lista_irrelevante).value_counts()\n",
    "frequencia_rel_irrelevantes = pd.Series(lista_irrelevante).value_counts(True)\n",
    "frequencia_rel_irrelevantes #frequ√™ncia relativa dos tweets irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.706587\n",
       "1    0.293413\n",
       "Name: Relev√¢ncia, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantidade_relevancia = treino.Relev√¢ncia.value_counts(True)\n",
    "quantidade_relevancia #porcentagem dos tweets que s√£o relevantes e irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nossa', 'eu', 'desviei', 'muito', 'r√°pido', 'de', 'um', 'spoiler', 'bacurau', 'me', 'senti', 'ninja', 'rt', 'avokdoido', 'quem', 'nao', 'gostou', '√©', 'no', 'm√≠nimo', 'do', 'pouco', 'nazista', 'sim', 'vi', 'ontem', 'com', 'a', 'viviane_cardoso', 'o', 'filme', 'fant√°stico', 'vale', 'pena', 'ver', 'se', 'voc√™', 'tiver', 'mente', 'aberta', 'e', 'qualquer', 'coisa', 'que', 'fa√ßo', 'vivi', 'sempre', 'bom', 'companheira', 'filmes', 'vida', 'lutas', 'lt', '3', 'hoje', 'finalmente', 'vejo', 'ser√°', 'consigo', 'chegar', 'at√©', 'sala', 'livre', 't√°', 'dif√≠cil', 'lzanin', 'texto', 'marcelo', 'coelho', 'chama', '‚Äòbolsonarisno', 'sinal', 'trocado‚Äô', 'lembra', 'ideia', 'alguns', 'jornalistas', 'na', '√©‚Ä¶', 'diretor', 'jo√£o', 'kleber', 'mendon√ßa', 'filho', 'n', 'vou', 'conseguir', 'qual', 'desse', 'q', 'galera', 'ta', 'doida', 'achei', 'era', 'peixe', 'isso', 'vai', 'assistir', 'sai', 'maluco', 'todo', 'dia', 'bora', 'galeres', 'aproveitando', 'pra', 'divulgar', 'melhor', '2019', 'personagem', 'tirei', 'lunga', 'a√≠', 'cara', 'j√°', 'assistiu', 'ùêâùêöÃÅ', 'ùêöùê¨ùê¨ùê¢ùê¨ùê≠ùê¢ùêÆ', 'ùêÅùêöùêúùêÆùê´ùêöùêÆ', 'jŒ¨', 'Œ±ssŒπsœÑŒπœÖ', 'Œ≤Œ±cœÖrŒ±œÖ', 'ùïµùñÜÃÅ', 'ùñÜùñòùñòùñéùñòùñôùñéùñö', 'ùï≠ùñÜùñàùñöùñóùñÜùñö', 'j·¥ÄÃÅ', '·¥Äss…™s·¥õ…™·¥ú', 'b·¥Ä·¥Ñ·¥ú Ä·¥Ä·¥úÔπñ', 'foda', 'omelete', 'ainda', 'foi', 'visto', 'por', 'mais', '130', 'mil', 'pessoas', 'diariosm', 'sucesso', 'p√∫blico', 'n√£o', 'deve', 'ser', 'exibido', 'nos', 'cinemas', 'santa', 'maria', 'aparentemente', 's√≥', 'ao', 'nesse', 'pa√≠s', 's√°bado', 'to', 'ansios√≠ssimaaaa', 'fds', 'passa', 'saindo', 'cedo', 'da', 'faculdade', 'correndo', 'kmendoncafilho', 'cine', 'arte', 'uff', 'uma', 'equipada', 'como', 'poucas', 'numa', 'universidade', 'p√∫blica', 'lotada', 'estudantes', 'para', 'brasi‚Ä¶', 'fazendo', 'propaganda', 'grandao', 'pro', 'uber', 'aqui', 'ele', 'disse', 'domingo', 'perfeito', 'poxa', 'algu√©m', 'leva', 'psousadealmeida', 'tenho', 'novo', 'tarantino', 'it', 'aaaaaa', 'favor', 'parem', 'falar', '√†s', '20', '40', 'chamem', 'sabe', 'bacurinha', 'thalitamari', '_meirabeatriz', 'menina', 'agora', 'tu', 'tava', 'falando', 'kkkkkk', 'apois', 'gente', 'faz', 'nem', 'treinamento', 'pq', 'esse', 'teria', 'oposi√ß√£o', 'queria', 'ter', 'dinheiro', 'wwwmlna', 'algum', 'homem', 'feminista', 'online', 'disposto', 'dar', 'professora', 'didatica', 'perguntando', 'turma', 'viu', 'mas', 'voc√™s', 'foram', 'gostei', 'tiduraes', 'em', '31¬∞', 'top', '250', 'letterboxd', 'lindo', 'comigo', 'anarcofino', 'minha', 'ex', 'cineasta', 'cheia', 'dos', 'contatos', 'manda', 'mensagem', 'digo', 'com‚Ä¶', 'hortawitch', 'assistindo', 'estou', 'vivendo', 'ou', 'apenas', 'curtindo', 'retwittando', 'repostando', 'coisas', 'sobre', 'baruchinha', 'tem', 'acho', 'amiga', 'pfvr', '2', 'pode', 'ganhar', 'pr√™mio', 'ano', 'beijos', 'sabia', 'karine', 'teles', 'dona', 'meu', 'c', 'susto', 'thread', 'deliciosa', 'tes√£o', 'estreia', '10de10', 'caazalberto', 'escreveu', 'momento', 'boca', 'garotada', 'incomodando', 'gado', 'üî•', 'üêÇ', 'review', 'completo', 'site', 'üëâ', 'affandre', 'tanto', 'invis√≠vel', 'ser√£o', 'exibidos', 'festival', 'toronto', 'grandes', 've√≠culos', 'temporada', 'premia√ß‚Ä¶', 'aconteceu', 'tudo', 'ruim', 'clap', 'existem', 'imperd√≠veis', 'pela', 'relev√¢ncia', 'tem√°tica', 'admir√°veis', 'pelo', 'significado', 'cultural', 'memor√°veis', 'serem', 'emblem√°ticos', 'tendo', 'monitoria', 'exposi√ß√£o', 'gringos', 'depois', 'matei', 'todos', 'sujei', 'os', 'quadros', 'sangue', 'tempos', 'sombrios', 'apreciar', 'boa', 'resist√™ncia', '07', 'setembro', 'po√ßos', 'caldas', 'estarei', 'l√°', 'ingressos', 'comprados', 'quero', 'mto', 'ir', 'caralhoo', 'preciso', 've', 'namoral', 'assistam', 'pqp', 'temia', 'veio', 'cin√©filo', 'perto', 'mim', 't√£o', 'more', 'bater', 'palma', 'tailand√™s', 'for', 'va', 'paz', 'casa', 'caralho', 'fica', 'come√ßa', 'pensar', 'cinema', 'cada', 'pensada', 'd√°', 'baixar', 'minhas', 'expectativas', 'porque', 'gosto', 'expectativa', 'alta', 'assim', 'ajuda', 'tempo', 'louvando', 'vcs', 'ficam', 'ai', 'oxewill94', 'vez', 'bicho', 'avengers', 'adoro', 'consumo', 'brasileiro', 'saco', 'cheio', 'pararem', 'fico', 'quando', 'falam', 'insistentemente', 'outra', 'diegoquaglia2', 'sei', 'odeio', 'black', 'mirror', 'compara√ß√£o', '‚Ä¶', 'constrangedor', 'existir', 'instagram', 'chamado', 'bacurau_memes', 'deus', 'fa√ßa', 'cair', 'tempi', 'ain', 'aguento', 'ouvir', 'esses', 'porra', 'entulhando', 'timeline', 'her√≥i', 'hollywood', 'ok', 'falo', 'nada', 'deixa', 'exaltar', 'daqui', 'pouquinho', 'tb', 'fazer', 'meus', 'amigos', 'assistirem', 'v√£o', 'vamos', 'la', 'mudar', 'pais', 'eh', 'possivel', 'eles', 'tenham', 'realmente', 'tirado', 'edir', 'macedo', 'passando', '50', 'semana', 'dias_ligia', 'ningu√©m', 'perguntou', 'ameei', 'amor', 'bolha', 'hype', 'est√°', 'maior', 'cmg', 'sido', 'lana', 'del', 'rey', 'harry', 'styles', 'errado', 'nenhum', 'deles', 'desisti', 'isto', 'capitulo', 'ova', 'apoiar', 'esganado', 'pelos', 'norte', 'americanos', 'kkkkkkkk', 'disseram', 'assisti', 'lo', 'vem', 'entrar', 'internet', 'alheia', 'outro', 'tbm', 'aquarius', 'renatoxavoso', 'kl√©ber', 'entre', 'coma', 'cu', 'toda', 'fam√≠lia', 'obrigado', 'demais', 'inuyalice', 'fui', '99', 'certeza', 'ia', 'sair', 'tiraram', 'yesterday', 'amigo', 'sess√£o', 'das', '9', 'teve', 'alguem', 'amanha', 'stargirllv', 'carai', 'b√™', 'indo', 'yuri_reinaldo', '16', 'horas', 'pensando', 'cinesiageek', 'deveria', 'elogio', 'tipo', 'm√≥', 'com√©dia', 'a√ß√£o', 'drama', 'it2', 'mt', 'ansiosa', 'principalmente', 'as', 'meninas', 'ü•≥', 'aula', 'vsffffffff', 'lugar', 'pros', 'lados', 'zona', 'madureira', 'shopping', 'rei', 'leao', 'programado', 'meier', 'sessao', '21h', 'noite', 'puta', 'excelente', 'saud√°vel', 'catarse', 'ali√°s', 'mesmo', 'oxigene', 'maoleskine', 'maratonar', 'breve', 'again', 'primeira', 'lan√ßamento', 'determina', 'quanto', 'cartaz', 'essa', 'lan√ßado', 'maravilhoso', 'mundo', 's√©rio', 'brasileiros', '‚ù§', 'Ô∏è', 'sopranine', 'ali', 'juntas', 'moskito', 'recebendo', 'mandar', 'grupo', 'dividido', 'amou', 'odiou', 'idosas', 'estavam', 'gostaram', 'admitiram', 'bem', 'feito', 'wotzik', 'brasil', 'urgente', 'vivam', 'necess√°rio', 'iskindolele', 'estava', 'obcecado', 'passava', '80', 'outros', 'torcia', 'falasse', 'sobre‚Ä¶', 'por√©m', 'afirmo', 'tranquilidade', 'milenio', 'veja', 'molena', 'incentive', 'nacional', 'meter', 'doido', 'logo', 'iguatemi', 'antes', 'fique', 'sem', 'minh', 'dando', 'palestra', 'pessoal', 'fez', 'trilha', 'sonora', 'mimos', 'pai', 'aquelacristiana', 'f√£s', '=', 'melhores', 'parece', 'infinitamente', 'interessante', 'rodrigoazo', 'olha', 'motivo', 'vermos', 'filma√ßo', 'ode', 'nosso', 'sobretudo', 'valente', 'povo', 'nordestino', 'sensa√ß√£o', 'reencontro', 'cultura', 'retrato', 'hist√≥rico', 'majestoso', 'viva', 'chegou', 'jundia√≠', 'sexo', 'sagrado', 'seu', 'corpo', 'templo', 'compartilh√°', 'louca', 'sozinha', 'msm', 'drag√£o', 'sigur_ross_', 'arrasta', 'maxxxramon', 'choque', 'tbt', 'assista', 'cr√≠tica', 'arrobanerd', 'longa', 'traz', 'reviravoltas', 'tirar', 'f√¥lego', 'paulo__junior__', 'ignorou', 'som', 'redor', 'principal', 'resistia', 'exatamente', 'rasteira', 'contradit√≥rio', 'nas', 'ideias', 'escreve', 'saber', 'met√°fora', 'p', 'gyn', 't√¥', 'lembrando', 'cena', 'fala', 'personagens', 'sob', 'efeito', 'forte', 'psicotr√≥pico', 'morrer', 'hauahuahuahuhauauuahuahuauuahuahhuahuauuahuahhuahuauuahuah', 'camisetinha', 'temos', 'so', '1', 'pessoa', 'reclamando', 'alguma', 'boyzinha', 'xuliaxx', 'zap', 'conversar', 'seguinte', 'eupalmeirensa', 'laianexx', 'laiane', 'quer', 'morenamoraes', 'abruxapreta', 'cinemarkoficial', 'querendo', 'salas', 'posso', 'abre', 'nenhuma', 'h4ckaq', 'dor', 'cabe√ßa', 'enjoo', 'vontade', 'ressaca', 'di√°logos', 'companhia', 'sincera', 'tomar', 'a√ßa√≠', 'verdade', 'beber', 'aff', 'sinopse', 'interessou', 'bastante', 'lucasnunnes97', '_msoliveira', 'processando', 'tanta', 'atualmente', 'gastei', 'chance', 'vendo', 'aquela', 'tranqueira', 'incr√≠vel', 'adjetivos', 'qualidade', 'deveriam', 'provavelmente', 'ja', 'cinemark', 'ent√£o', 'cuiabano', 'prestigiar', 'hein', 'faltava', 'unisse', 'todas', 'tribos', 'pablomoreno', 'amei', 'sert√£o', 'impacto', 'lucidez', 'realiza√ß√£o', 'precisa', 'digerir', 'ap√≥s', 'muitos', 'elogios', 'espero', 'encante', 'hj', 'assisto', 'refrescos', 'agenda', 'conturbada', 'par√ßa', '‚ô•', 'meio', 'loucura', 'assistido', 'mandei', 'tweet', 'mauromendoncaf', 'engravidar', 'louco', 'tesao', 'sendo', 'socorro', 'dizer', '‚Äúamiga', 'juntas‚Äù', 'supero', 'aniversario', 'paga', '√∫nica', 'entendi', 'ns', 'this', 'is', 'homofobia', 'filhudi', 'filhusi', 'brasilia', 'w', 'f√©', 'poder', 'fernanda', 'reservando', 's√£o', 'luiz', 'alunos', 'ü§ß', 'ü•∞', 'üò≠', '√≥bvio', 'enfiar', 'obra', 'inspira', 'maravilha', 'silveropereira', 'votei', 'assistiria', 'hora', 'penso', 'perplexo', 'recorte', 'irregular', 'obsess√µes', 'exporta√ß√£o', 'comentar', 'realidade', 'amigas', 'essas', 'hist√≥rias', 'itimalian', 'bacurit', 'kkkkk', 'aaa', 'filha', 'pois', 'trate', 'cinem', 'volta', 'agoraaaaaaa', 'viaverdeshop', 'amiguinho', 'queira', 'prefer√™ncia', 'segunda', 'quarta', 'pre√ßo', 'promocional', 'postando', 'plena', 'convic√ß√£o', 'flop', 'citar', 'correntes', 'al√©m', 'central', 'ela', 'amanh√£', 'mesa', 'buteco', 'centro', 'cmggg', 'algm', 'hd5trange', 'nice', 'comprando', 'ingresso', 'naquelas', 'm√°quinas', 'deu', 'gritar', 'film√£o', 'caray', 'onde', 'refletir', 'vir√°', 'escolas', 'museus', 'sentido', 'literal', 'figurado', 'tentar', 'recuperar', 'projeto', 'entregar', 'rever', 'dressasantiago_', 'diferenciado', 'chamaram', 'estamos', 'evoluindo', 'caionare4l', 'vim', 'humildemente', 'irem', 'assitir', 'perfeitooo', 'iamplanett', 'aten√ß√£o', '22', '09', 'passar', 'vit√≥ria', 'op√ß√£o', 'barata', 'local‚Ä¶', 'bacuraufilme', 'fa√ßam', 'teste', 'domingas', 'orgulho', 'objetivos', 'cumpridos', 'beareclama', 'kkkk', 'falta', 'pacote', 'teresa', 'pospunkcearense', 'tabacaria', 'imposs√≠vel', 'inagaki', 'virar', 'sou', 'via', 'davirocha', 'bluecoloredboy', 'conte', 'amo', 'socorr', 'kkkkkkk', 'tirou', 'pr√≥pria', 'cidade', 'amig√°vel', 'pac√≠fica', 'tentando', 'atrapalhar', 'sua', 'obrigada', 'pr√≥prias', 'r√©deas', 'destino', 'continuar', 'sobrevivendo', 'omegamark_xii', 'te', 'agostinho', 'carrara', 'matar', 'carara', 'matem√°tica', 'basica', 'marque', 'ami‚Ä¶', 'ifavmarkten', 'tive', 'pesquisar', 'google', 'vc', 'escrevendo', 'bacalhau', 'heinhein', 'sdds', 'umazinha', 'ficar', 'alterada', 'johnforfans', 'vamo', 'tarde', 'nadar', 'minas', 'dps', 'f1', 'comer', 'sorvete', 'conversando', 'yearofsilencce', 'mia', 'links', 'favoooorre', 'c0breira', 'easter', 'eggs', 'cobreiros', 'papai', 'respons√°vel', 'daquele', 'buraco', 'cia', 'cavaram', 'o‚Ä¶', 'ogrunhido', 'confundo', 'bator√©', 'personalidade', 'miss√£o', 'nesta', 'capaz', 'cima', 'cumprir', 'seus', 'seja', 'quais', 'forem', 'arrasa', 'visual', 'aplique', 'achando', '√≥timo', 'ouvi', 'uns', '‚Äúnossa', 'agora‚Äú', 'te‚Ä¶', 'contemplada', 'caravana', 'caiu', 'l√°grima', 'gulagcanavieiro', 'pontua', 'acuradamente', 'infelizmente', 'sudeste', 'merda', 'ps', 'yuribt', 'camisa', 'parmera', '2a', 'compartilhe', 'grande', 'abra√ßo', '‚ö°', 'ü§Ø', 'üõ∏', 'üç∑', 'capa', 'edi√ß√£o', 'cahiers', 'du', 'cin√©ma', 'revista', 'importante', 'traz‚Ä¶', 'refletindo', 'cakespacek', 'v√°', 'wiiz', 'pedrinhofonseca', 'primeiro', 'fim', 'r', '5mi', 'arrecadados', 'durante', 'grava√ß√µes', '800', 'empregos', 'gerados', 'lend√°rio', 'uuuuu', 'flu√≠', 'bacanal', 'perfeita', 'b_soviet', 'abrutaflor', 'piranha', 'galinha', 'tidal', '√©pico', 'greggui_', 'pen√∫ltimo', 'lista', 'produ√ß√£o', 'franco', 'venceu', 'j√∫ri', 'cannes', 'tornand‚Ä¶', 'ah', 'justo', 'claro', 'mtvbrasil', 'arrecada', '5', 'milh√£o', 'bilheteria', 'final', 'gt', 'voz', 'conhecimento', 'raz√£o', 'consulta', 'importa', 'verdadeira', 'inspira√ß√£o', 'banhadaaouro', 'bus√£o', 'madrugada', 'issoeomuso', 'resposta', 'moreirapaty', 'inveja', 'caso', 'inclui', 'tamb√©m', 'hahahahah', 'digno', 'choca', 'rondelicia', 'finja', 'aturar', 'viado', 'enaltecendo', 'riverdale', 'enaltecer', 'gepeto666', 'p_dromenezes', 'somos', 'regi√£o', 'tecnologicamente', 'avan√ßada', 'buscando', 'disputar', 'posi√ß√µes', 'regi√µes', 'cl√°ssico', 'xen√≥fobo', 'acham', 'diferenciada', 'paulista', 'carioca', 'testes_damassa', 'fiquei', 'vingadores', 'aviso', 'virmos', 'po‚Ä¶', 'babi', 'colen', 'üëèüèº', 'üíó', '‚Äúvoc√™', '‚Äù', 'ma', 'bosta', 'testinhos', 'buzzfeed', 'üíÅüèª', 'v√™', 'n4rja', 'palavra', 'entender', 'exemplo', 'b√™bada', 'ide‚Ä¶', 'm_i_n_u_s', 'acabei', 'tweets', 'aleatorios', 'sudestino', 'ate', 'bahia', 'nordeste', 'desmerecer', 'uahauahaha', 'axzgazzoni', 'num', 'aqueles', '√¥nibus', 'presentear', 'sweet', 'coffee', 'week', 'pr√≥xima', 'banco', 'camilamotad', 'sonia', 'braga', 'comprei', 'porta', 'crise', 'ansiedade', 'embora', 'chorando', 'caminho', 'td', 'partir', 'quinta', 'feira', 'sesc', 'rua', 'augusta', 'sp', 'projetado', '4k', 'sonz√£o', '30‚Ä¶', 'fiz', 'üòÇ', 'quest√£o', 'invas√µes', 'domina√ß√µes', 'viol√™ncia', 'parte', 'construiu', 'invadiu', 'popula√ß√£o', 'morava', 'dizimada', 'nome', 'civiliza√ß√£o', 'pesquisa', 'r√°pida', 'caralhoooo', 'cancelada', 'cad√™', 'chega', 'chernobabe', '‚Äúpo', 'assiste', 'bacurau‚Äù', 'olhe', 'peh', 'usando', 'sandalia', 'couro', 'midianinja', '‚Äúbacurau‚Äù', 'ilustra', '‚Äúcahiers', 'cin√©ma‚Äù', 'conceituada', 'francesa', 'conh‚Ä¶', 'vetromn', 'cidades', 'dnlnblgng', 'menos', 'respeito', 'g√≠ria', 'sexual', 'boto', 'pezinhos', 'fora', 'carlos', 'come√ßam', 'exibir', 'boazinha', 'esperar', 'ccamls', 'sobrevivend9', 'esperan√ßa', 'isura_eru', 'fanart', 'mikhaetc', 'conquistas', 'juri', 'jovem', 'transar', 'querer', 'assassinar', 'gringo', 'filadaputa', 'ldna', 'tinha', 'hahaha', 'impressionada', 'in‚Ä¶', 'mostra', 'derrotar', 'ataques', 'bolson√°rios', 'conversaafiada', 'inteira', 'carapu√ßa', 'serviu', 'dontcallmealf', 'acredito', 'piracicaba', 'enquanto', 'falacioso', 'segue', '2‚Ä¶', 'tjxciv', '24h', 'chamei', 'tia', 'combinamos', 'chegamos', 'descobrimos', 'virou', 'programa√ß√£o', 'resultado', 'passado', 'condena', 'netrlix', 'jdornelles', 'oficial', 'dispon√≠vel', 'spotify', 'deezer', 'apple', 'music', 'play', 'üéª', 'üé∫', 'ü•Å', 'üéº', '_', 'n√°usea', 'pulo_', 'beijar', 'dormir', 'acordar', 'feirinha', 'pq10', 'senhor', 'sondas', 'ü§©', 'üòç', 'putz', 'diria', 'candymel', 'üéµ', 'completa', 'escutar', 'fi‚Ä¶', 'izarcosta', 'fosse', 'acionar', 'meme', 'juliancampos', 'caf√©', 'recebi', 'sach√™', 'a√ß√∫car', 'entra', '200', 'üí•', 'deixe', 'f‚Ä¶', 'heisenboff', 'ultima', 'brasileira', 'este', 'nervosa', '~~bacurau', 'mirror~~', 'conseguindo', 'cineart', 'boulos', 'chegando', 'metr√≥polis', 'coincid√™ncia', 'segundo', 'votos', 'internacionais', 'cinco', 'd√©cada', 'parasita', 'aranha', 'aran‚Ä¶', 'tehpipimi', 'irmao', 'desde', '2014', 'reclama‚Ä¶', 'j0anacs', 'high', 'life', 'shecomesincolo1', '√≠cone', 'berrando', 'perguntas', 'bacurauverso', 'acabou', 'trailer', 'tv', 'lot√©rica', 'desses', 'reforma', 'previd√™ncia', 'algo', 'comassim', 'just√≠ssimo', 'mano', 'dessa', 'voltar', 'unha', 'homenagear', 'femesmo', 'bastou', 'for√ßar', 'associa√ß√£o', 'absolutamente', 'kkkkkkkkkkkkkkkkkkkkkkkkk', 'alveskath', 'certo', 'üòÅ', 'medo', 'levantar', '√°gua', 'cozinha', 'alicepa', 'imagina', 'branco', 'americano', 'diferente', 'torrents', '[eu', 'mesma', 'bibliotec√°ria', 'refer√™ncia]', 'bacurinho', 'dei', 'boas', 'risadas', 'parabens', 'üëç', 'kdstephanie', '√∫ltimos', 'sabendo', 'lidar', 'badgesucks', 'enxergarmos', 'prisma', 'sociedade', 'vivemos', 'iremos', 'compreender', 'proposta', '_o_gugga', 'igual', 'overdose', 'emocomrade', 'intenso', 'vitrine_filmes', 'mail', 'show', 'maca√©', 'pedi', 'passarem', 'üòü', 'jazzbfr', 'quase', 'nunca', 'custa', 'meia', 'obriga', 'presen√ßa', 'ela‚Ä¶', 'n√©', 'isent√£o', 'esp√©cie', 'tropa', 'elite', 'esquerda', 'uso', 'cat√°rtica‚Ä¶', 'playlist', 'virginiaamoon', '√©poca', 'araputanga', 'apelido', 'super', 'engra√ßado', 'co', 'descobrir', 'hahahahahahahahaha', 'taurina', 'pr√≥prio', '10', 'crime', 'camaradas', 'fan', 'merchand', 'ator', 'prefeito', 'tony', 'j√∫nior', 'curtiu', 'comentou', 'desenho', 'feliz', 'dms', 'rmovinup', 'amoooooo', 'euzinha', '‚ô°', 'diariope', 'mata', 'tudinho', 'faca', 'oficialmente', 'botei', 'foder', 'rainha', 'teco', 'beb√™s', 'zadorarocha', 'comprar', 'guarda', 'chuva', 'ielison_', 'al√¥', 'campina', 'hor√°rios', '15', '35', 'pamonha', 'curau', 'dou', 'passarinho', 'ddaenerys', '‚Äúassiste', 'pagar', 'limpar', 'disso', 'xenofobico', 'pe√ßo', 'ruimdrigo_', 'rima', 'cosmopolita', 'conquistando', 'jamais', 'esquece', 'suas', 'ra√≠zes', 'perde', 'liga√ß√£o', 'srra', 'esfor√ßo', 'dificuldade', 'cidad√£o', 'andreense', 'hor√°rio', '22h30', 'carro', 'qu√™', 'pobre', 'neg√≥cio', 'saiu', 'raivaaa', 'aquele', 'f', 'gregoriorrr', 'sulista', 'bola', 'tira', 'belissimo', 'sarro', 'sulist‚Ä¶', 'delicia', '√≥', 'capitalismo', 'matastes', 'grito', 'povoado', 'chegaram', 'masterclass', 'editor', 'facha', 'critiquei', 'esta', 'aaaaaaaaaaaa', 'parar', 'i', 'got', 'silvero', 'pereira', 'yanplm123', 'men', 'teu', 'insta', 'melted', 'v√≠deos', 'buzzfeedbrasil', 'quiz', 'honra', 'meupai', 'pega', 'come', 'paulomoreria', 'ciladapieper', 'anigos', 'paia√ßada', 'ahahahah', 'pior', 'concordo', 'resumo', 'vibes', 'vomite', 'saio', 'atirando', 'mesmooooo', 'vir', 'evildebora', 'evildani13', 'cansadae', 'juliexndrews', '√≥dio', 'pantanal', 'passada', 'tinha‚Ä¶', 'tatifers', 'papocultura', 'hist√≥ria', 'cobra', 'colunista', 'sihan', 'felix', 'blog', 'papo', '√†', 'torna', 'gosta', 'asas', 'longe', 'p√°ssaro', 'territ√≥rio', 'acaba', 'voltando', 'fiel', 'coragem', 'trouxa', 'inventa', 'encher', 'move', 'mundos', 'fundos', 'defender', 'entes', 'queridos', 'cert√≠ssimo', 'conta', 'detalhe', 'receber√°', 'dm', 'mandando', 'shipneide', 'mista', 'vanity', 'fair', 'excessivamente', 'estereotipados', 'pegou', 'shade', 'nan']\n"
     ]
    }
   ],
   "source": [
    "palavras_totais = [] #lista com todas as palavras sem duplicatas\n",
    "\n",
    "for palavra in lista_relativa:\n",
    "    if palavra not in palavras_totais:\n",
    "        palavras_totais.append(palavra)\n",
    "print(palavras_totais)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfei√ßoamento:\n",
    "\n",
    "Os trabalhos v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separa√ß√£o de espa√ßos entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o ou classifica√ß√£o\n",
    "* Criar categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que n√£o posso usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza an√°lise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
