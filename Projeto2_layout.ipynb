{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Beatriz Muniz de Castro e Silva\n",
    "\n",
    "Nome: Nicole Sarvasi Alves da Costa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "#Instalando o pacote emoji para limpar mensagens\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#importando os pacotes necessários\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import functools\n",
    "import operator\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: @nicknennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @nickcnennis\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'bacurau'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 700\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 500\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 401",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ef43b50e526d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#retira os retweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproduto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'extended'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretweeted\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'RT'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmsgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTweepError\u001b[0m: Twitter error response: status code = 401"
     ]
    }
   ],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "\n",
    "#retira os retweets\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode='extended').items():  \n",
    "    if (not msg.retweeted) and ('RT' not in msg.full_text): \n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa é manual. Faça a mesma pelo Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leitura do arquivo excel\n",
    "treino = pd.read_excel(\"bacurau.xlsx\", \"Treinamento\") #tabela da parte de treinamento do excel\n",
    "teste = pd.read_excel(\"bacurau.xlsx\", \"Teste\") #tabela da parte de teste do excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    import string\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if not word.startswith('https'))\n",
    "    \n",
    "    text_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "    text_split_whitespace = [substr.split() for substr in text_split_emoji]\n",
    "    text_split = functools.reduce(operator.concat, text_split_whitespace)\n",
    "    text = ' '.join(word for word in text_split)\n",
    "    \n",
    "    punctuation = '[!-/.:?;@]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    nova_linha = '[\\n]'\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed = re.sub(nova_linha, \" \", text_subbed)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar uma lista com os tweets de teste limpos (sem @ e essas negocias loucas)\n",
    "teste_t = pd.Series(teste[\"Teste\"])\n",
    "teste_limpo = []\n",
    "for i in teste_t:\n",
    "    teste_limpo.append(cleanup(i.lower()))\n",
    "# teste_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar uma lista com os tweets de treinamento limpos (same as above)\n",
    "treino_s = pd.Series(treino[\"Treinamento\"])\n",
    "treino_limpo = []\n",
    "for i in treino_s:\n",
    "    treino_limpo.append(cleanup(str(i).lower()))\n",
    "# treino_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau       0.059806\n",
       "de            0.044272\n",
       "e             0.021100\n",
       "rt            0.021100\n",
       "a             0.019288\n",
       "é             0.017864\n",
       "que           0.017735\n",
       "tirei         0.015534\n",
       "o             0.015146\n",
       "eu            0.012816\n",
       "você          0.011133\n",
       "do            0.009968\n",
       "ver           0.009191\n",
       "pra           0.008932\n",
       "no            0.008026\n",
       "um            0.007896\n",
       "assistir      0.007896\n",
       "filme         0.007249\n",
       "não           0.007120\n",
       "uma           0.006861\n",
       "com           0.006731\n",
       "para          0.006731\n",
       "em            0.006084\n",
       "da            0.005955\n",
       "tem           0.005696\n",
       "mas           0.005437\n",
       "gente         0.005437\n",
       "qual          0.004919\n",
       "domingas      0.004531\n",
       "lunga         0.004531\n",
       "                ...   \n",
       "tirar         0.000129\n",
       "esta          0.000129\n",
       "aquela        0.000129\n",
       "pensada       0.000129\n",
       "cadê          0.000129\n",
       "molena        0.000129\n",
       "vivi          0.000129\n",
       "buteco        0.000129\n",
       "outro         0.000129\n",
       "cmg           0.000129\n",
       "ninja         0.000129\n",
       "gravações     0.000129\n",
       "arrecada      0.000129\n",
       "temia         0.000129\n",
       "matastes      0.000129\n",
       "sorvete       0.000129\n",
       "desmerecer    0.000129\n",
       "10            0.000129\n",
       "últimos       0.000129\n",
       "super         0.000129\n",
       "shopping      0.000129\n",
       "escreve       0.000129\n",
       "derrotar      0.000129\n",
       "falacioso     0.000129\n",
       "deles         0.000129\n",
       "incentive     0.000129\n",
       "ódio          0.000129\n",
       "repostando    0.000129\n",
       "senhor        0.000129\n",
       "ova           0.000129\n",
       "Length: 1693, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino_t = (\" \").join(treino_limpo) #junta todos os tweets em uma string só\n",
    "lista_relativa = treino_t.split() #divide a string pra fazer uma lista com cada palavra separadamente\n",
    "frequencia_absoluta = pd.Series(lista_relativa).value_counts()\n",
    "frequencia_relativa = pd.Series(lista_relativa).value_counts(True)\n",
    "frequencia_relativa #frequencia relativa total das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau            0.059778\n",
       "de                 0.034291\n",
       "que                0.027340\n",
       "e                  0.024096\n",
       "o                  0.018999\n",
       "pra                0.018536\n",
       "ver                0.018072\n",
       "a                  0.016682\n",
       "assistir           0.014365\n",
       "filme              0.013438\n",
       "é                  0.012975\n",
       "do                 0.012975\n",
       "eu                 0.012512\n",
       "no                 0.010658\n",
       "não                0.009268\n",
       "um                 0.007414\n",
       "em                 0.007414\n",
       "mas                0.006951\n",
       "cinema             0.006951\n",
       "com                0.006951\n",
       "mais               0.006487\n",
       "me                 0.006487\n",
       "tem                0.005561\n",
       "uma                0.005097\n",
       "sobre              0.005097\n",
       "bom                0.005097\n",
       "vou                0.005097\n",
       "da                 0.004634\n",
       "vai                0.004634\n",
       "q                  0.004634\n",
       "                     ...   \n",
       "existem            0.000463\n",
       "centro             0.000463\n",
       "sido               0.000463\n",
       "paga               0.000463\n",
       "novoalguém         0.000463\n",
       "caionare4l         0.000463\n",
       "todas              0.000463\n",
       "resistia           0.000463\n",
       "maxxxramon         0.000463\n",
       "assistido          0.000463\n",
       "reservando         0.000463\n",
       "abruxapreta        0.000463\n",
       "elogios            0.000463\n",
       "aula               0.000463\n",
       "9alguem            0.000463\n",
       "companheira        0.000463\n",
       "ingresso           0.000463\n",
       "nenhuma            0.000463\n",
       "bora               0.000463\n",
       "maoleskine         0.000463\n",
       "psousadealmeida    0.000463\n",
       "aliás              0.000463\n",
       "emblemáticos       0.000463\n",
       "obra               0.000463\n",
       "americanos         0.000463\n",
       "verdade            0.000463\n",
       "deu                0.000463\n",
       "_msoliveira        0.000463\n",
       "rey                0.000463\n",
       "ova                0.000463\n",
       "Length: 878, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel = treino.index[treino.Relevância == 1] #critério para selecionar só os tweets relevantes\n",
    "treino_r = treino.loc[rel, \"Treinamento\"]  #cria nova database com os tweets relevantes\n",
    "\n",
    "\n",
    "#frequência das palavras nos tweets relevantes\n",
    "treino_rt = treino_r.str.cat()\n",
    "treino_rt = cleanup(treino_rt.lower())\n",
    "lista_relevante = treino_rt.split()\n",
    "frequencia_absoluta = pd.Series(lista_relevante).value_counts()\n",
    "frequencia_rel_relevantes = pd.Series(lista_relevante).value_counts(True)\n",
    "frequencia_rel_relevantes #frequência relativa dos tweets relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bacurau            0.054954\n",
       "de                 0.051858\n",
       "é                  0.021091\n",
       "a                  0.020511\n",
       "e                  0.020511\n",
       "que                0.014899\n",
       "tirei              0.014319\n",
       "você               0.014319\n",
       "o                  0.014125\n",
       "eu                 0.011997\n",
       "do                 0.009481\n",
       "para               0.009094\n",
       "um                 0.008707\n",
       "uma                0.007933\n",
       "no                 0.007546\n",
       "com                0.007159\n",
       "da                 0.006966\n",
       "domingas           0.006772\n",
       "qual               0.006772\n",
       "lunga              0.006579\n",
       "não                0.006385\n",
       "gente              0.006192\n",
       "em                 0.005998\n",
       "personagem         0.005998\n",
       "tem                0.005998\n",
       "assistir           0.005805\n",
       "ver                0.005805\n",
       "pra                0.005611\n",
       "se                 0.005418\n",
       "filme              0.005224\n",
       "                     ...   \n",
       "coragem            0.000193\n",
       "rondelicia         0.000193\n",
       "tropa              0.000193\n",
       "cineart            0.000193\n",
       "água               0.000193\n",
       "responsável        0.000193\n",
       "resposta           0.000193\n",
       "delicia            0.000193\n",
       "juri               0.000193\n",
       "inventa            0.000193\n",
       "bacuraujusto       0.000193\n",
       "instagram          0.000193\n",
       "nomemedo           0.000193\n",
       "beijar             0.000193\n",
       "conseguindo        0.000193\n",
       "got                0.000193\n",
       "quadros            0.000193\n",
       "boazinha           0.000193\n",
       "brasi…por          0.000193\n",
       "comigort           0.000193\n",
       "gregoriorrr        0.000193\n",
       "certeza            0.000193\n",
       "kkkksó             0.000193\n",
       "vanity             0.000193\n",
       "mirror~~menina     0.000193\n",
       "𝕵𝖆́                0.000193\n",
       "catártica…carai    0.000193\n",
       "exportação         0.000193\n",
       "façam              0.000193\n",
       "kkkkk              0.000193\n",
       "Length: 1244, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irel = treino.index[treino.Relevância == 0]\n",
    "treino_ir = treino.loc[irel, \"Treinamento\"]  #tweets irrelevantes\n",
    "\n",
    "#frequência das palavras nos tweets irrelevantes\n",
    "treino_irt = treino_ir.str.cat()\n",
    "treino_irt = cleanup(treino_irt.lower())\n",
    "lista_irrelevante = treino_irt.split()\n",
    "frequencia_absoluta = pd.Series(lista_irrelevante).value_counts()\n",
    "frequencia_rel_irrelevantes = pd.Series(lista_irrelevante).value_counts(True)\n",
    "frequencia_rel_irrelevantes #frequência relativa dos tweets irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.706587\n",
       "1    0.293413\n",
       "Name: Relevância, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantidade_relevancia = treino.Relevância.value_counts(True)\n",
    "quantidade_relevancia #porcentagem dos tweets que são relevantes e irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nossa', 'eu', 'desviei', 'muito', 'rápido', 'de', 'um', 'spoiler', 'bacurau', 'me', 'senti', 'ninja', 'rt', 'avokdoido', 'quem', 'nao', 'gostou', 'é', 'no', 'mínimo', 'do', 'pouco', 'nazista', 'sim', 'vi', 'ontem', 'com', 'a', 'viviane_cardoso', 'o', 'filme', 'fantástico', 'vale', 'pena', 'ver', 'se', 'você', 'tiver', 'mente', 'aberta', 'e', 'qualquer', 'coisa', 'que', 'faço', 'vivi', 'sempre', 'bom', 'companheira', 'filmes', 'vida', 'lutas', 'lt', '3', 'hoje', 'finalmente', 'vejo', 'será', 'consigo', 'chegar', 'até', 'sala', 'livre', 'tá', 'difícil', 'lzanin', 'texto', 'marcelo', 'coelho', 'chama', '‘bolsonarisno', 'sinal', 'trocado’', 'lembra', 'ideia', 'alguns', 'jornalistas', 'na', 'é…', 'diretor', 'joão', 'kleber', 'mendonça', 'filho', 'n', 'vou', 'conseguir', 'qual', 'desse', 'q', 'galera', 'ta', 'doida', 'achei', 'era', 'peixe', 'isso', 'vai', 'assistir', 'sai', 'maluco', 'todo', 'dia', 'bora', 'galeres', 'aproveitando', 'pra', 'divulgar', 'melhor', '2019', 'personagem', 'tirei', 'lunga', 'aí', 'cara', 'já', 'assistiu', '𝐉𝐚́', '𝐚𝐬𝐬𝐢𝐬𝐭𝐢𝐮', '𝐁𝐚𝐜𝐮𝐫𝐚𝐮', 'jά', 'αssιsτιυ', 'βαcυrαυ', '𝕵𝖆́', '𝖆𝖘𝖘𝖎𝖘𝖙𝖎𝖚', '𝕭𝖆𝖈𝖚𝖗𝖆𝖚', 'jᴀ́', 'ᴀssɪsᴛɪᴜ', 'bᴀᴄᴜʀᴀᴜ﹖', 'foda', 'omelete', 'ainda', 'foi', 'visto', 'por', 'mais', '130', 'mil', 'pessoas', 'diariosm', 'sucesso', 'público', 'não', 'deve', 'ser', 'exibido', 'nos', 'cinemas', 'santa', 'maria', 'aparentemente', 'só', 'ao', 'nesse', 'país', 'sábado', 'to', 'ansiosíssimaaaa', 'fds', 'passa', 'saindo', 'cedo', 'da', 'faculdade', 'correndo', 'kmendoncafilho', 'cine', 'arte', 'uff', 'uma', 'equipada', 'como', 'poucas', 'numa', 'universidade', 'pública', 'lotada', 'estudantes', 'para', 'brasi…', 'fazendo', 'propaganda', 'grandao', 'pro', 'uber', 'aqui', 'ele', 'disse', 'domingo', 'perfeito', 'poxa', 'alguém', 'leva', 'psousadealmeida', 'tenho', 'novo', 'tarantino', 'it', 'aaaaaa', 'favor', 'parem', 'falar', 'às', '20', '40', 'chamem', 'sabe', 'bacurinha', 'thalitamari', '_meirabeatriz', 'menina', 'agora', 'tu', 'tava', 'falando', 'kkkkkk', 'apois', 'gente', 'faz', 'nem', 'treinamento', 'pq', 'esse', 'teria', 'oposição', 'queria', 'ter', 'dinheiro', 'wwwmlna', 'algum', 'homem', 'feminista', 'online', 'disposto', 'dar', 'professora', 'didatica', 'perguntando', 'turma', 'viu', 'mas', 'vocês', 'foram', 'gostei', 'tiduraes', 'em', '31°', 'top', '250', 'letterboxd', 'lindo', 'comigo', 'anarcofino', 'minha', 'ex', 'cineasta', 'cheia', 'dos', 'contatos', 'manda', 'mensagem', 'digo', 'com…', 'hortawitch', 'assistindo', 'estou', 'vivendo', 'ou', 'apenas', 'curtindo', 'retwittando', 'repostando', 'coisas', 'sobre', 'baruchinha', 'tem', 'acho', 'amiga', 'pfvr', '2', 'pode', 'ganhar', 'prêmio', 'ano', 'beijos', 'sabia', 'karine', 'teles', 'dona', 'meu', 'c', 'susto', 'thread', 'deliciosa', 'tesão', 'estreia', '10de10', 'caazalberto', 'escreveu', 'momento', 'boca', 'garotada', 'incomodando', 'gado', '🔥', '🐂', 'review', 'completo', 'site', '👉', 'affandre', 'tanto', 'invisível', 'serão', 'exibidos', 'festival', 'toronto', 'grandes', 'veículos', 'temporada', 'premiaç…', 'aconteceu', 'tudo', 'ruim', 'clap', 'existem', 'imperdíveis', 'pela', 'relevância', 'temática', 'admiráveis', 'pelo', 'significado', 'cultural', 'memoráveis', 'serem', 'emblemáticos', 'tendo', 'monitoria', 'exposição', 'gringos', 'depois', 'matei', 'todos', 'sujei', 'os', 'quadros', 'sangue', 'tempos', 'sombrios', 'apreciar', 'boa', 'resistência', '07', 'setembro', 'poços', 'caldas', 'estarei', 'lá', 'ingressos', 'comprados', 'quero', 'mto', 'ir', 'caralhoo', 'preciso', 've', 'namoral', 'assistam', 'pqp', 'temia', 'veio', 'cinéfilo', 'perto', 'mim', 'tão', 'more', 'bater', 'palma', 'tailandês', 'for', 'va', 'paz', 'casa', 'caralho', 'fica', 'começa', 'pensar', 'cinema', 'cada', 'pensada', 'dá', 'baixar', 'minhas', 'expectativas', 'porque', 'gosto', 'expectativa', 'alta', 'assim', 'ajuda', 'tempo', 'louvando', 'vcs', 'ficam', 'ai', 'oxewill94', 'vez', 'bicho', 'avengers', 'adoro', 'consumo', 'brasileiro', 'saco', 'cheio', 'pararem', 'fico', 'quando', 'falam', 'insistentemente', 'outra', 'diegoquaglia2', 'sei', 'odeio', 'black', 'mirror', 'comparação', '…', 'constrangedor', 'existir', 'instagram', 'chamado', 'bacurau_memes', 'deus', 'faça', 'cair', 'tempi', 'ain', 'aguento', 'ouvir', 'esses', 'porra', 'entulhando', 'timeline', 'herói', 'hollywood', 'ok', 'falo', 'nada', 'deixa', 'exaltar', 'daqui', 'pouquinho', 'tb', 'fazer', 'meus', 'amigos', 'assistirem', 'vão', 'vamos', 'la', 'mudar', 'pais', 'eh', 'possivel', 'eles', 'tenham', 'realmente', 'tirado', 'edir', 'macedo', 'passando', '50', 'semana', 'dias_ligia', 'ninguém', 'perguntou', 'ameei', 'amor', 'bolha', 'hype', 'está', 'maior', 'cmg', 'sido', 'lana', 'del', 'rey', 'harry', 'styles', 'errado', 'nenhum', 'deles', 'desisti', 'isto', 'capitulo', 'ova', 'apoiar', 'esganado', 'pelos', 'norte', 'americanos', 'kkkkkkkk', 'disseram', 'assisti', 'lo', 'vem', 'entrar', 'internet', 'alheia', 'outro', 'tbm', 'aquarius', 'renatoxavoso', 'kléber', 'entre', 'coma', 'cu', 'toda', 'família', 'obrigado', 'demais', 'inuyalice', 'fui', '99', 'certeza', 'ia', 'sair', 'tiraram', 'yesterday', 'amigo', 'sessão', 'das', '9', 'teve', 'alguem', 'amanha', 'stargirllv', 'carai', 'bê', 'indo', 'yuri_reinaldo', '16', 'horas', 'pensando', 'cinesiageek', 'deveria', 'elogio', 'tipo', 'mó', 'comédia', 'ação', 'drama', 'it2', 'mt', 'ansiosa', 'principalmente', 'as', 'meninas', '🥳', 'aula', 'vsffffffff', 'lugar', 'pros', 'lados', 'zona', 'madureira', 'shopping', 'rei', 'leao', 'programado', 'meier', 'sessao', '21h', 'noite', 'puta', 'excelente', 'saudável', 'catarse', 'aliás', 'mesmo', 'oxigene', 'maoleskine', 'maratonar', 'breve', 'again', 'primeira', 'lançamento', 'determina', 'quanto', 'cartaz', 'essa', 'lançado', 'maravilhoso', 'mundo', 'sério', 'brasileiros', '❤', '️', 'sopranine', 'ali', 'juntas', 'moskito', 'recebendo', 'mandar', 'grupo', 'dividido', 'amou', 'odiou', 'idosas', 'estavam', 'gostaram', 'admitiram', 'bem', 'feito', 'wotzik', 'brasil', 'urgente', 'vivam', 'necessário', 'iskindolele', 'estava', 'obcecado', 'passava', '80', 'outros', 'torcia', 'falasse', 'sobre…', 'porém', 'afirmo', 'tranquilidade', 'milenio', 'veja', 'molena', 'incentive', 'nacional', 'meter', 'doido', 'logo', 'iguatemi', 'antes', 'fique', 'sem', 'minh', 'dando', 'palestra', 'pessoal', 'fez', 'trilha', 'sonora', 'mimos', 'pai', 'aquelacristiana', 'fãs', '=', 'melhores', 'parece', 'infinitamente', 'interessante', 'rodrigoazo', 'olha', 'motivo', 'vermos', 'filmaço', 'ode', 'nosso', 'sobretudo', 'valente', 'povo', 'nordestino', 'sensação', 'reencontro', 'cultura', 'retrato', 'histórico', 'majestoso', 'viva', 'chegou', 'jundiaí', 'sexo', 'sagrado', 'seu', 'corpo', 'templo', 'compartilhá', 'louca', 'sozinha', 'msm', 'dragão', 'sigur_ross_', 'arrasta', 'maxxxramon', 'choque', 'tbt', 'assista', 'crítica', 'arrobanerd', 'longa', 'traz', 'reviravoltas', 'tirar', 'fôlego', 'paulo__junior__', 'ignorou', 'som', 'redor', 'principal', 'resistia', 'exatamente', 'rasteira', 'contraditório', 'nas', 'ideias', 'escreve', 'saber', 'metáfora', 'p', 'gyn', 'tô', 'lembrando', 'cena', 'fala', 'personagens', 'sob', 'efeito', 'forte', 'psicotrópico', 'morrer', 'hauahuahuahuhauauuahuahuauuahuahhuahuauuahuahhuahuauuahuah', 'camisetinha', 'temos', 'so', '1', 'pessoa', 'reclamando', 'alguma', 'boyzinha', 'xuliaxx', 'zap', 'conversar', 'seguinte', 'eupalmeirensa', 'laianexx', 'laiane', 'quer', 'morenamoraes', 'abruxapreta', 'cinemarkoficial', 'querendo', 'salas', 'posso', 'abre', 'nenhuma', 'h4ckaq', 'dor', 'cabeça', 'enjoo', 'vontade', 'ressaca', 'diálogos', 'companhia', 'sincera', 'tomar', 'açaí', 'verdade', 'beber', 'aff', 'sinopse', 'interessou', 'bastante', 'lucasnunnes97', '_msoliveira', 'processando', 'tanta', 'atualmente', 'gastei', 'chance', 'vendo', 'aquela', 'tranqueira', 'incrível', 'adjetivos', 'qualidade', 'deveriam', 'provavelmente', 'ja', 'cinemark', 'então', 'cuiabano', 'prestigiar', 'hein', 'faltava', 'unisse', 'todas', 'tribos', 'pablomoreno', 'amei', 'sertão', 'impacto', 'lucidez', 'realização', 'precisa', 'digerir', 'após', 'muitos', 'elogios', 'espero', 'encante', 'hj', 'assisto', 'refrescos', 'agenda', 'conturbada', 'parça', '♥', 'meio', 'loucura', 'assistido', 'mandei', 'tweet', 'mauromendoncaf', 'engravidar', 'louco', 'tesao', 'sendo', 'socorro', 'dizer', '“amiga', 'juntas”', 'supero', 'aniversario', 'paga', 'única', 'entendi', 'ns', 'this', 'is', 'homofobia', 'filhudi', 'filhusi', 'brasilia', 'w', 'fé', 'poder', 'fernanda', 'reservando', 'são', 'luiz', 'alunos', '🤧', '🥰', '😭', 'óbvio', 'enfiar', 'obra', 'inspira', 'maravilha', 'silveropereira', 'votei', 'assistiria', 'hora', 'penso', 'perplexo', 'recorte', 'irregular', 'obsessões', 'exportação', 'comentar', 'realidade', 'amigas', 'essas', 'histórias', 'itimalian', 'bacurit', 'kkkkk', 'aaa', 'filha', 'pois', 'trate', 'cinem', 'volta', 'agoraaaaaaa', 'viaverdeshop', 'amiguinho', 'queira', 'preferência', 'segunda', 'quarta', 'preço', 'promocional', 'postando', 'plena', 'convicção', 'flop', 'citar', 'correntes', 'além', 'central', 'ela', 'amanhã', 'mesa', 'buteco', 'centro', 'cmggg', 'algm', 'hd5trange', 'nice', 'comprando', 'ingresso', 'naquelas', 'máquinas', 'deu', 'gritar', 'filmão', 'caray', 'onde', 'refletir', 'virá', 'escolas', 'museus', 'sentido', 'literal', 'figurado', 'tentar', 'recuperar', 'projeto', 'entregar', 'rever', 'dressasantiago_', 'diferenciado', 'chamaram', 'estamos', 'evoluindo', 'caionare4l', 'vim', 'humildemente', 'irem', 'assitir', 'perfeitooo', 'iamplanett', 'atenção', '22', '09', 'passar', 'vitória', 'opção', 'barata', 'local…', 'bacuraufilme', 'façam', 'teste', 'domingas', 'orgulho', 'objetivos', 'cumpridos', 'beareclama', 'kkkk', 'falta', 'pacote', 'teresa', 'pospunkcearense', 'tabacaria', 'impossível', 'inagaki', 'virar', 'sou', 'via', 'davirocha', 'bluecoloredboy', 'conte', 'amo', 'socorr', 'kkkkkkk', 'tirou', 'própria', 'cidade', 'amigável', 'pacífica', 'tentando', 'atrapalhar', 'sua', 'obrigada', 'próprias', 'rédeas', 'destino', 'continuar', 'sobrevivendo', 'omegamark_xii', 'te', 'agostinho', 'carrara', 'matar', 'carara', 'matemática', 'basica', 'marque', 'ami…', 'ifavmarkten', 'tive', 'pesquisar', 'google', 'vc', 'escrevendo', 'bacalhau', 'heinhein', 'sdds', 'umazinha', 'ficar', 'alterada', 'johnforfans', 'vamo', 'tarde', 'nadar', 'minas', 'dps', 'f1', 'comer', 'sorvete', 'conversando', 'yearofsilencce', 'mia', 'links', 'favoooorre', 'c0breira', 'easter', 'eggs', 'cobreiros', 'papai', 'responsável', 'daquele', 'buraco', 'cia', 'cavaram', 'o…', 'ogrunhido', 'confundo', 'batoré', 'personalidade', 'missão', 'nesta', 'capaz', 'cima', 'cumprir', 'seus', 'seja', 'quais', 'forem', 'arrasa', 'visual', 'aplique', 'achando', 'ótimo', 'ouvi', 'uns', '“nossa', 'agora“', 'te…', 'contemplada', 'caravana', 'caiu', 'lágrima', 'gulagcanavieiro', 'pontua', 'acuradamente', 'infelizmente', 'sudeste', 'merda', 'ps', 'yuribt', 'camisa', 'parmera', '2a', 'compartilhe', 'grande', 'abraço', '⚡', '🤯', '🛸', '🍷', 'capa', 'edição', 'cahiers', 'du', 'cinéma', 'revista', 'importante', 'traz…', 'refletindo', 'cakespacek', 'vá', 'wiiz', 'pedrinhofonseca', 'primeiro', 'fim', 'r', '5mi', 'arrecadados', 'durante', 'gravações', '800', 'empregos', 'gerados', 'lendário', 'uuuuu', 'fluí', 'bacanal', 'perfeita', 'b_soviet', 'abrutaflor', 'piranha', 'galinha', 'tidal', 'épico', 'greggui_', 'penúltimo', 'lista', 'produção', 'franco', 'venceu', 'júri', 'cannes', 'tornand…', 'ah', 'justo', 'claro', 'mtvbrasil', 'arrecada', '5', 'milhão', 'bilheteria', 'final', 'gt', 'voz', 'conhecimento', 'razão', 'consulta', 'importa', 'verdadeira', 'inspiração', 'banhadaaouro', 'busão', 'madrugada', 'issoeomuso', 'resposta', 'moreirapaty', 'inveja', 'caso', 'inclui', 'também', 'hahahahah', 'digno', 'choca', 'rondelicia', 'finja', 'aturar', 'viado', 'enaltecendo', 'riverdale', 'enaltecer', 'gepeto666', 'p_dromenezes', 'somos', 'região', 'tecnologicamente', 'avançada', 'buscando', 'disputar', 'posições', 'regiões', 'clássico', 'xenófobo', 'acham', 'diferenciada', 'paulista', 'carioca', 'testes_damassa', 'fiquei', 'vingadores', 'aviso', 'virmos', 'po…', 'babi', 'colen', '👏🏼', '💗', '“você', '”', 'ma', 'bosta', 'testinhos', 'buzzfeed', '💁🏻', 'vê', 'n4rja', 'palavra', 'entender', 'exemplo', 'bêbada', 'ide…', 'm_i_n_u_s', 'acabei', 'tweets', 'aleatorios', 'sudestino', 'ate', 'bahia', 'nordeste', 'desmerecer', 'uahauahaha', 'axzgazzoni', 'num', 'aqueles', 'ônibus', 'presentear', 'sweet', 'coffee', 'week', 'próxima', 'banco', 'camilamotad', 'sonia', 'braga', 'comprei', 'porta', 'crise', 'ansiedade', 'embora', 'chorando', 'caminho', 'td', 'partir', 'quinta', 'feira', 'sesc', 'rua', 'augusta', 'sp', 'projetado', '4k', 'sonzão', '30…', 'fiz', '😂', 'questão', 'invasões', 'dominações', 'violência', 'parte', 'construiu', 'invadiu', 'população', 'morava', 'dizimada', 'nome', 'civilização', 'pesquisa', 'rápida', 'caralhoooo', 'cancelada', 'cadê', 'chega', 'chernobabe', '“po', 'assiste', 'bacurau”', 'olhe', 'peh', 'usando', 'sandalia', 'couro', 'midianinja', '“bacurau”', 'ilustra', '“cahiers', 'cinéma”', 'conceituada', 'francesa', 'conh…', 'vetromn', 'cidades', 'dnlnblgng', 'menos', 'respeito', 'gíria', 'sexual', 'boto', 'pezinhos', 'fora', 'carlos', 'começam', 'exibir', 'boazinha', 'esperar', 'ccamls', 'sobrevivend9', 'esperança', 'isura_eru', 'fanart', 'mikhaetc', 'conquistas', 'juri', 'jovem', 'transar', 'querer', 'assassinar', 'gringo', 'filadaputa', 'ldna', 'tinha', 'hahaha', 'impressionada', 'in…', 'mostra', 'derrotar', 'ataques', 'bolsonários', 'conversaafiada', 'inteira', 'carapuça', 'serviu', 'dontcallmealf', 'acredito', 'piracicaba', 'enquanto', 'falacioso', 'segue', '2…', 'tjxciv', '24h', 'chamei', 'tia', 'combinamos', 'chegamos', 'descobrimos', 'virou', 'programação', 'resultado', 'passado', 'condena', 'netrlix', 'jdornelles', 'oficial', 'disponível', 'spotify', 'deezer', 'apple', 'music', 'play', '🎻', '🎺', '🥁', '🎼', '_', 'náusea', 'pulo_', 'beijar', 'dormir', 'acordar', 'feirinha', 'pq10', 'senhor', 'sondas', '🤩', '😍', 'putz', 'diria', 'candymel', '🎵', 'completa', 'escutar', 'fi…', 'izarcosta', 'fosse', 'acionar', 'meme', 'juliancampos', 'café', 'recebi', 'sachê', 'açúcar', 'entra', '200', '💥', 'deixe', 'f…', 'heisenboff', 'ultima', 'brasileira', 'este', 'nervosa', '~~bacurau', 'mirror~~', 'conseguindo', 'cineart', 'boulos', 'chegando', 'metrópolis', 'coincidência', 'segundo', 'votos', 'internacionais', 'cinco', 'década', 'parasita', 'aranha', 'aran…', 'tehpipimi', 'irmao', 'desde', '2014', 'reclama…', 'j0anacs', 'high', 'life', 'shecomesincolo1', 'ícone', 'berrando', 'perguntas', 'bacurauverso', 'acabou', 'trailer', 'tv', 'lotérica', 'desses', 'reforma', 'previdência', 'algo', 'comassim', 'justíssimo', 'mano', 'dessa', 'voltar', 'unha', 'homenagear', 'femesmo', 'bastou', 'forçar', 'associação', 'absolutamente', 'kkkkkkkkkkkkkkkkkkkkkkkkk', 'alveskath', 'certo', '😁', 'medo', 'levantar', 'água', 'cozinha', 'alicepa', 'imagina', 'branco', 'americano', 'diferente', 'torrents', '[eu', 'mesma', 'bibliotecária', 'referência]', 'bacurinho', 'dei', 'boas', 'risadas', 'parabens', '👍', 'kdstephanie', 'últimos', 'sabendo', 'lidar', 'badgesucks', 'enxergarmos', 'prisma', 'sociedade', 'vivemos', 'iremos', 'compreender', 'proposta', '_o_gugga', 'igual', 'overdose', 'emocomrade', 'intenso', 'vitrine_filmes', 'mail', 'show', 'macaé', 'pedi', 'passarem', '😟', 'jazzbfr', 'quase', 'nunca', 'custa', 'meia', 'obriga', 'presença', 'ela…', 'né', 'isentão', 'espécie', 'tropa', 'elite', 'esquerda', 'uso', 'catártica…', 'playlist', 'virginiaamoon', 'época', 'araputanga', 'apelido', 'super', 'engraçado', 'co', 'descobrir', 'hahahahahahahahaha', 'taurina', 'próprio', '10', 'crime', 'camaradas', 'fan', 'merchand', 'ator', 'prefeito', 'tony', 'júnior', 'curtiu', 'comentou', 'desenho', 'feliz', 'dms', 'rmovinup', 'amoooooo', 'euzinha', '♡', 'diariope', 'mata', 'tudinho', 'faca', 'oficialmente', 'botei', 'foder', 'rainha', 'teco', 'bebês', 'zadorarocha', 'comprar', 'guarda', 'chuva', 'ielison_', 'alô', 'campina', 'horários', '15', '35', 'pamonha', 'curau', 'dou', 'passarinho', 'ddaenerys', '“assiste', 'pagar', 'limpar', 'disso', 'xenofobico', 'peço', 'ruimdrigo_', 'rima', 'cosmopolita', 'conquistando', 'jamais', 'esquece', 'suas', 'raízes', 'perde', 'ligação', 'srra', 'esforço', 'dificuldade', 'cidadão', 'andreense', 'horário', '22h30', 'carro', 'quê', 'pobre', 'negócio', 'saiu', 'raivaaa', 'aquele', 'f', 'gregoriorrr', 'sulista', 'bola', 'tira', 'belissimo', 'sarro', 'sulist…', 'delicia', 'ó', 'capitalismo', 'matastes', 'grito', 'povoado', 'chegaram', 'masterclass', 'editor', 'facha', 'critiquei', 'esta', 'aaaaaaaaaaaa', 'parar', 'i', 'got', 'silvero', 'pereira', 'yanplm123', 'men', 'teu', 'insta', 'melted', 'vídeos', 'buzzfeedbrasil', 'quiz', 'honra', 'meupai', 'pega', 'come', 'paulomoreria', 'ciladapieper', 'anigos', 'paiaçada', 'ahahahah', 'pior', 'concordo', 'resumo', 'vibes', 'vomite', 'saio', 'atirando', 'mesmooooo', 'vir', 'evildebora', 'evildani13', 'cansadae', 'juliexndrews', 'ódio', 'pantanal', 'passada', 'tinha…', 'tatifers', 'papocultura', 'história', 'cobra', 'colunista', 'sihan', 'felix', 'blog', 'papo', 'à', 'torna', 'gosta', 'asas', 'longe', 'pássaro', 'território', 'acaba', 'voltando', 'fiel', 'coragem', 'trouxa', 'inventa', 'encher', 'move', 'mundos', 'fundos', 'defender', 'entes', 'queridos', 'certíssimo', 'conta', 'detalhe', 'receberá', 'dm', 'mandando', 'shipneide', 'mista', 'vanity', 'fair', 'excessivamente', 'estereotipados', 'pegou', 'shade', 'nan']\n"
     ]
    }
   ],
   "source": [
    "palavras_totais = [] #lista com todas as palavras sem duplicatas\n",
    "\n",
    "for palavra in lista_relativa:\n",
    "    if palavra not in palavras_totais:\n",
    "        palavras_totais.append(palavra)\n",
    "print(palavras_totais)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
